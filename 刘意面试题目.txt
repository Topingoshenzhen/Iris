
Q:带linux的项目开发流程



1，因为开发前面经过架构，需求分析有了一个完整框架以及各个模块，组长先把整个框架以及模块上传到例如github，SSH，组员根据自己的任务把这个项目里自己的负责的功能模块git pull下来

2，需求分析完成并且制定需求文档

3，把需求文档转换成接口文档给组员进行各个模块开发

4，组员根据功能进行开发，在本地开发，因为开发在支线，完成开发合并到git主线

5，开发，单元测试都可以在windows环境，甚至可以在windows环境启动项目然后跟进报错

6，通过Docker部署在linux[部署到tomcat？]，如果有错误通过读取Docker容器日志debug，日志名称是Dockerlogs + 容器ID

7，部署完成后测试包括SIT测试【系统集成测试】，UAT测试【用户验收测试】，这些在linux部署运行，用SSH来远程登录服务器来调试或者跟进生产问题，在项目路径里存有日志，cd命令进入路径，
   然后vim 文件名进入文件，通过grep，tail命令查看日志文件。

8，

代码的日志文件路径可以自定义，一般定义在项目目录里面，docker部署到k8s，url，前台启动查看日志，后台看日志。


 git user：go-roo


使用git需要明白这几个名词，工作区[本地仓库在的文件夹]，暂存区[add操作将代码加到这里]，版本库[commit操作将代码从暂存区放到这里]，远端[理解为github，push将代码从版本库推到github]

本地仓库操作：
git init    初始化一个文件夹为本地仓库[文件夹下面建立了一个.git文件夹]，这里可以用可以不用，因为git clone直接把整个仓库搬运下来，并且直接git status可以看到当前处于master分支
git status  查看当前项目状态，处于什么分支，master分支都是最终版本的代码


git add . / 文件名  这里把全部文件 / 特定文件名 加入暂存区，一般直接用git add -A全部增加，删除，改动的代码都添加到暂存区，表示关注这些代码

git commit -m '注释' [也叫提交/快照]这里将暂存区里的东西提交到当前分支？的版本库？，m等于注释用来区别每一次的提交，
git push    这里把版本库代码直接同步到远端github,因为现在处于master分支，是直接push到github，如果在其他分支应该用push origin master将代码同步到主分支

git log     查看日志，查看每一个提交的日期，作者，comment id等

git pull    可以拉取代码到本地

git diff    比较工作区，暂存区的差异

git diff -cached 比较暂存区与版本库的差异

git differ HEAD 查看在最后一次提交之后的所有变更

git differ --staged 查看添加（add）而并未提交（commit）的变更


本地仓库建立好了，需要与GitHub上远程仓库链接起来，两者之间的传输是通过SSH加密的，应该是私钥加密吧，具体流程参考网上，我这里不分本地远程仓库操作，因为开发者操作应该都在本地仓库执行


git最重要的应该是分支，冲突这些。


pull & push
上传本地仓库内容至远程分支
git push -u origin master
第一次推送master分支时，加上了-u参数，在以后的推送或者拉取时就可以简化命令。这些为啥与我们见到不太同，因为同步到master分支

拉取远程仓库中别人的修改，应该是远程分支比较准确
git pull origin master


创建分支clean_up:  git branch clean_up

跳转分支clean_up:  git checkout -b clean_up  一般只用这个创建并且直接跳转到目标分支，省略了第一步。git checkout -b [branchname]命令

跳转回master分支:  git branch master，应该已经创建了，这个命令就是直接跳转到目标分支

执行合并[merge]:   将在clean_up分支上的修改合并到master上：git merge clean_up

删除分支clan_up:   git branch -d clean_up


git rebase命令：这个命令在开发中用到的比较高频，只要不是一个人搞开发，基本都会用到，这个命令的使用场景如下：比如现在同时在master分支上创建了一个分支A，和B，分别搞开发任务A，B；等A任务开发结束，提交A并且合并到master后，这时候如果B任务也开发结束了，要提交的话，仅仅通过push想合并到master是会起冲突的，因为这时候的B分支已经不是基于最新的master分支了（因为从master分支创建B分支后，master分支又合并了一个A），所以这时候想要解决的话就要用到rebase命令了：

功能 	                                命令
基于master或者commitId进行rebase到最新 	git rebase -i master/commitId
继续rebase 	                        git rebase --continue
跳过rebase 	                        git rebase --skip
停止rebase 	                        git rebase --abort

一般我们有冲突后，需要解决冲突，然后git add下，然后调用git rebase --continue命令，一般就会提示成功了，这时候就可以继续操作了。


git stash命令： 使用场景是：有时候我们正在开发一个任务，突然有一个bug需要修改，我们当前的代码怎么办？方法有很多，可以commit一下，等会reset回来；也可以stash命令操作，这里着重分析一下stash命令：

功能 	                                命令
切换到其它分支前保存当前工作区代码 	git stash
然后切换回来的时候 	                git stash pop
详细保存当前代码 	                git stash save [commit content]
显示保存的stash列表 	                git stash list
恢复保存的某一个进度 	                git stash apply stash@{n}
删除某一个stash标记 	                git stash drop stash@{n}
清除stash 	                        git stash clear


git pull 将代码从Github拉取下来，如果两个人同时对某一文件相同部分修改了，其中一人提交到github了，另一个
个人这时候git pull该文件下了到自己本地，此时，代码依旧拉取下来，
不过会有提示如下：

	<<<<<<<<<<<<Head
	
         github里的部分
	
         ===============
	
         自己的部分
	
        <<<<<<<<<<<<  
这时候你需要区域同事讨论该修改是否是正确，在Git中，用 HEAD 表示当前版本


实际上连接到github就是通过SSH,不过这个要配置，麻烦。


版本回退：推荐用revert，不推荐reset


，git revert comment id / git reset comment id，因为


git tag 使用场景：tag是里程碑的意思，一般我们会在版本发布的时候对这一段的代码进行里程碑操作，使用git tag命令：

功能 	        命令
创建tag 	git tag -m [msg] [tagname] [commit]
查看tag列表 	git tag -l 或者 git tag --list
删除tag 	git tag -d [tagname]
推送tag到远端 	git push origin  [tagname]

**Note:**我们在这里打的tag是不会随着push branchname命令直接就推送到远端的，原因很简单，如果这样的话我们的tag库岂不是都被大家搞的很乱，只有在使用针对tag的push才会将tag信息同步到远端，就是上面表格中最后一个命令。


部署在docker，容器启动成功，docker也有日志可以跟进，Docker logs + 容器ID，

用SSH来远程登录服务器，这时候面试很多时候有个问题，如果部署有出现过什么bug么？

部署了多少台服务器，上线后怎么查看数据流动？


git branch master feature-1 feature-2
git remote origin

在feature-1怎么切换到feature-2分支，然后rebase master到feature-2分支

git branch feature-2

git rebase master --continue



在feature-1，落后于origin的master分支，请拉取origin的内容，然后merge到当前分支




   linux开发怎么调试bug，解决生产问题？



Q：代码审核的意义
   等于代码cross review，一是检查代码隐藏错误，因为每个人发现错误角度不一样，而且开发者容易进入误区自己开发的没问题。二是让各个开发者进一步了解其他人的业务以及代码，做好人员备份便于后期维护。





Q：API限速设计

,例如给该用户在redis加入用户id的调用api次数，在调用api之前进行检查。如果超过了就返回MSG：操作频繁请重试。
   现在有一个rest api表示的资源，/v1/users。设计几个RESTPI，形式为http_method path并且写出在哪里传递参数





Q：http包里客户端调用服务端api接口






Q：设计千万级，秒级精度的后台定时启动程序
go + 时间轮




Q：微服务里如果有请求过来谁来接收并且分流，gp-micro架构
微服务里前后端分离，请求如果是HTTP请求过来，整个项目怎么处理请求，用的什么格式，json么，请求是由一个服务来接收还是直接到某一个具体服务

go-micro可以创建三种服务：web服务，API服务，srv服务。

微服务整个项目怎么创建了，例如自己的模块文件夹怎么建立，是用micro new命令来安装micro环境？？





Q：consul怎么实现服务的注册以及发现，每个服务器都部署么？
consul集群，server-leader检查,其他server进行服务发现？
   consul对于服务发现的标志就是服务名，这个机制有问题就是如果服务名相同就会出现问题。

   一个consul做中心调度，服务器宕机后还是高可用么？--》一个服务器部署consul做调度   


Q：微服务？


Q：Mysql,Nosql区别 = 关系型，非关系型数据库的区别
？

Mysql：

1.Mysql是存储于硬盘的，数据结构为表结构，结构稳定便于查询，体积小、速度快、成本低；

2.不过数据库容易出现性能瓶颈，要做分表分库、主从复制、异构复制等等优化；

3.可以开启事务处理—保持数据的一致性；

4.由于以标准化为前提，数据更新的开销很小（相同的字段基本上只有一处）；

5.可以进行Join等复杂查询。


nosql： 

1. 该数据库是基于内存的，取速度是磁盘读取速度的几十倍到上百倍，减少了时间和空间上的开销，却又很难保证数据一致性。数据结构为key-value类型，比较随意。




2. 高扩展，简单的扩展：典型例子是Cassandra，由于其架构是类似于经典的P2P，所以能通过轻松地添加新的节点来扩展这个集群; 

3. 高性能，快速的读写：主要例子有Redis，由于其逻辑简单，而且纯内存操作，使得其性能非常出色，单节点每秒可以处理超过10万次读写操作; 

4. 高可用，低廉的成本：这是大多数分布式数据库共有的特点，因为主要都是开源软件，没有昂贵的License成本; 


Q：Mysql里explain用法，Docker命令
？
explain select语句，可以调试sql语句性能，返回结构有十多个字段，每个字段代表不同的意义。



Q：索引面试问题？

为什么用索引？
   数据库中最常见的慢查询优化方式是给字段加索引。因为索引其实就是一种优化查询的数据结构，比如Mysql中的索引是用B+树实现的，而B+树就是一种数据结构，可以优化查询速度，可以利用索引快速查找数据，所以能优化查询。哈希表、完全平衡二叉树、B树、B+树等等数据结构都可以提高查询速度

索引分为几类？
   索引分为普通索引，
   唯一约束索引  [索引值必须唯一，不可重复，可以为空值]，
   全文索引  [用CREATE FULLTEXT INDEX可以设置全文索引]

，
   单列索引  [只有一个字段]
   多列索引  [多个字段上建一个索引，必须按照顺序来使用否则失效，例如a,b,c组合索引，只能用a a,b a,b,c a,c来查询，其中a, a,c用的索引一致]
   空间索引  [使用SPATIAL参数可以设置空间索引。只能建立在控件数据类型（LINESTRING、POINT、GEOMETRY等）上，这样可以提高系统获取空间数据的效率，且该字段不能为空值]

建立索引的语法？
   CREATE INDEX indexname ON tablename(colume(length)) [如果colume是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length]

   ALTER TABLE tablename ADD INDEX indexname(colume) [修改表结构添加索引]

   CREATE TABLE tablename(
        ID     int NOT NULL,
        user   VARCHAR(16) NOT NULL, 
        INDEX  indexname (username(length)) 
   )engine=Innodb;                                [在建表的时候指定索引]

   DROP INDEX indexname ON tablename;

   CREATE UNIQUE INDEX indexname ON tablename(colume(length)) [建立唯一索引，该索引值必须唯一，不可重复，可以为空值]

索引选取原则？[数据量较大的表建立索引，小数据量建立索引影响性能。主键，外键建立索引，值变化较少的字段，经常有修改的字段不要建立索引，索引不超过五个最好]

1. 在经常用作过滤器的字段上建立索引。

2. 在SQL语句中经常进行GROUP BY、ORDER BY的字段上建立索引。

3. 在不同值较少的字段上不必要建立索引，如性别字段。

4. 对于经常存取的列避免建立索引。

5. 用于联接的列（主健/外健）上建立索引。

6. 确定针对该表的操作是大量的查询操作还是大量的增删改操作。 

7. 建立索引帮助特定的查询。检查自己的sql语句，为那些频繁在where子句中出现的字段建立索引。 

8. 尝试建立复合索引提高系统性能。修改复合索引将消耗更长时间，同时，复合索引也占磁盘空间。 

9. 对于小型的表，建立索引可能会影响性能。

10. 应该避免对具有较少值的字段进行索引。

11. 避免选择大型数据类型的列作为索引。


Q：InnoDB，MyIsam这几种数据引擎的区别？

   在MySQL5.5以前使用的是MyISAM数据库引擎，在5.6开始引入了InnoDB数据库引擎，MyISAM更适合读密集的表，而InnoDB更适合写密集的的表[例如在数据库的主从分离中，可以选择从服务器使用MyISAM做为引擎]。

MyISAM

    MyISAM只支持表锁，不支持行锁。 在操作MyISAM表时，会对操作的整张表加锁，读锁和写锁是互斥的，读锁是共享锁，写锁是排他锁。表锁的锁细粒度大，发生锁冲突的概率高，并发度最低，等待时间太长。
    不支持事务。 强调的是性能，每次查询具有原子性，执行速度比InnoDB快，但是不提供事务支持。
    不支持外键，支持全文索引[整张表可以建索引，读取速度非常快]
    不支持崩溃恢复 （崩溃恢复指的是，数据库宕机后，原先操作一半的情况需要进行恢复，重新写入磁盘）
    保存了表的总行数，使用count函数会直接取出该值，但是使用where后则还是需要遍历获取。
    MyISAM支持压缩，可以降低磁盘占用。

InnoDB

    支持行锁 使用了MVCC技术来实现高并发,如果并发写操作较多时应该选择InnoDB。
    支持事务 InnoDB和MyISAM相比最大的特色就是支持了ACID事务,保证数据的正确性，支持事务的提交（commit）和回滚（rollback）。
    支持外键，不支持全文索引 全文索引可以使用第三方的插件
    支持崩溃后的安全 恢复

注意，同一个数据库也可以使用多种存储引擎的表。如果一个表要求比较高的事务处理，可以选择InnoDB。这个数据库中可以将查询要求比较高的表选择MyISAM存储。如果该数据库需要一个用于查询的临时表，可以选择MEMORY存储引擎。在数据库的主从分离中，可以选择从服务器使用MyISAM做为引擎。


ISAM：ISAM是一个定义明确且历经时间考验的数据表格管理方法，它在设计之时就考虑到数据库被查询的次数要远大于更新的次数。因此，ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。ISAM的两个主要不足之处在于，它不支持事务处理，也不能够容错：如果你的硬盘崩溃了，那么数据文件就无法恢复了。如果你正在把ISAM用在关键任务应用程序里，那就必须经常备份你所有的实时数据，通过其复制特性，MYSQL能够支持这样的备份应用程序。


MyISAM：MyISAM是MySQL的ISAM扩展格式和缺省的数据库引擎。除了提供ISAM里所没有的索引和字段管理的大量功能，MyISAM还使用一种表格锁定的机制，来优化多个并发的读写操作，其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新机制所浪费的空间。MyISAM还有一些有用的扩展，例如用来修复数据库文件的MyISAMCHK工具和用来恢复浪费空间的 MyISAMPACK工具。MYISAM强调了快速读取操作，这可能就是为什么MySQL受到了WEB开发如此青睐的主要原因：在WEB开发中你所进行的大量数据操作都是读取操作。所以，大多数虚拟主机提供商和INTERNET平台提供商只允许使用MYISAM格式。MyISAM格式的一个重要缺陷就是不能在表损坏后恢复数据。插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比较低，也可以使用。


InnoDB：InnoDB数据库引擎都是造就MySQL灵活性的技术的直接产品，这项技术就是MYSQL+API。在使用MYSQL的时候，你所面对的每一个挑战几乎都源于ISAM和MyISAM数据库引擎不支持事务处理（transaction process）也不支持外来键。尽管要比ISAM和 MyISAM引擎慢很多，但是InnoDB包括了对事务处理和外来键的支持，这两点都是前两个引擎所没有的。如前所述，如果你的设计需要这些特性中的一者或者两者，那你就要被迫使用后两个引擎中的一个了。支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。 


MEMORY: MEMORY是MySQL中一类特殊的存储引擎。它使用存储在内存中的内容来创建表，而且数据全部放在内存中。这些特性与前面的两个很不同。每个基于MEMORY存储引擎的表实际对应一个磁盘文件。该文件的文件名与表名相同，类型为frm类型。该文件中只存储表的结构。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。值得注意的是，服务器需要有足够的内存来维持MEMORY存储引擎的表的使用。如果不需要了，可以释放内存，甚至删除不需要的表。MEMORY默认使用哈希索引。速度比使用B型树索引快。当然如果你想用B型树索引，可以在创建索引时指定。注意，MEMORY用到的很少，因为它是把数据存到内存中，如果内存出现异常就会影响数据。如果重启或者关机，所有数据都会消失。因此，基于MEMORY的表的生命周期很短，一般是一次性的。所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择MEMOEY。它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。


MyISAM：默认的MySQL插件式存储引擎，它是在Web、数据仓储和其他应用环境下最常使用的存储引擎之一。注意，通过更改STORAGE_ENGINE配置变量，能够方便地更改MySQL服务器的默认存储引擎。
InnoDB：用于事务处理应用程序，具有众多特性，包括ACID事务支持。(提供行级锁)
BDB：可替代InnoDB的事务引擎，支持COMMIT、ROLLBACK和其他事务特性。
Memory：将所有数据保存在RAM中，在需要快速查找引用和其他类似数据的环境下，可提供极快的访问。
Merge：允许MySQL DBA或开发人员将一系列等同的MyISAM表以逻辑方式组合在一起，并作为1个对象引用它们。对于诸如数据仓储等VLDB环境十分适合。
Archive：为大量很少引用的历史、归档、或安全审计信息的存储和检索提供了完美的解决方案。
Federated：能够将多个分离的MySQL服务器链接起来，从多个物理服务器创建一个逻辑数据库。十分适合于分布式环境或数据集市环境。
Cluster/NDB：MySQL的簇式数据库引擎，尤其适合于具有高性能查找要求的应用程序，这类查找需求还要求具有最高的正常工作时间和可用性。
Other：其他存储引擎包括CSV（引用由逗号隔开的用作数据库表的文件），Blackhole（用于临时禁止对数据库的应用程序输入），以及Example引擎（可为快速创建定制的插件式存储引擎提供帮助）。


Q：WebSocket与Socket区别

WebSocket 是一种在单个 TCP 连接上进行全双工通信的协议。WebSocket 使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据。在 WebSocket API 中，浏览器和服务器只需要完成一次 HTTP 握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。
一般情况下我们使用 HTTP 有一个很大的缺陷，就是 HTTP 只能由客户端来主动发起，如果有需要服务端主动通知的业务，就需要轮询。轮询的效率低，非常浪费资源。
为了解决 Web 端即时通讯的需求就出现了 WebSocket。

WebSocket 与 Socket 的区别：
Socket 是传输控制层的接口。用户可以通过 Socket 来操作底层 TCP/IP 协议族通信。
WebSocket 是一个完整应用层协议。
Socket 更灵活，WebSocket 更易用。
两者都能做即时通讯。





Q：TCP与UDP的区别

TCP：
有状态，传输慢，安全性高，适用于少量加密数据通信


UDP：
无状态，传输快，安全性低，适用于大量数据通信，允许数据容错的场景，例如即时通信，微信聊天


两者最根本区别是，UDP不需要对每一个请求进行应答，TCP需要对每一个请求应答，并且没有收到应答就等待一定时间重发请求，这里涉及到最大报文响应时间再议

  TCP/IP传送一个很大的包，怎么传？分包




Q：go语言并发怎么做？并发的管理方式？
go语言级别支持并发，就是利用channel，有缓冲通道异步通信，无缓冲通道同步通信。如果是跨平台通讯，可以用队列来并发。通道关闭后，写入该通道有错误，如果从其中读取为0




Q：go语言里如果用panic，函数退出了报异常，现在不想退出怎么处理？

一般panic, defer, recover一起使用，panic异常在执行之前会按照规则执行defer，然后在defer里使用recover可以接收panic错误信息然后跳到其他地方不结束函数。

func main(){

	defer f(){
		if err := recover(); err != nil {
			fmt.Println("hehe")
			//这里可以跳转至其他位置
		}
	}()
	/*代码段*/
	panic("错误信息")
}




Q：Token，生产者消费者模式，
Token不熟，生产者消费者类型不太熟
   Token验证

   生产者消费者模型，这里涉及到各种锁[例如waitGroup，sync.Mutex]，通道，select机制？





Q：网络连接的半开半闭怎么做，TCP是双工的，[这问题为啥百度不出来]





Q：TCP七层协议模型[这个有答案]，分层TCP编程怎么编程[这问题为啥百度不出来]？
   




Q：OOP面向对象编程理解？
   万物皆可为对象，对对象进行编程。
   所有事物都可以抽象为对象[具有属性以及行为]，我们将对象的属性和行为（方法）统一到一个“类”中。然后实例化类，即规定对象特定的属性和方法。具体的对象又可以完成一系列不同的行为，这就是面向对象编程。
   
   它有三个特征:封装、继承、多态
   封装就是说隐藏对象的属性和实现细节，仅对外提供公共访问方式（接口），我们可以对内部实现细节进行更改，对外接口不变。调用该接口，就可以实现更改后的功能。
   继承就是有父类和子类，子类可以拥有父类的属性和方法。
   多态就是建立了一个类，通过实例化类，就可以给对象分配不同的属性和方法，这样就形成了很多在属性和方法上存在差异的对象，这就是多态。



Q：go语言里关于时间的应用
   time.Sleep(time.Second * 2)  休息两秒钟 
   TimeNow := time.Now()   结果为：2020-01-20 15:16:05.5382136 +0800 CST m=+0.001000001
   Time := time.Now().UnixNano() 结果为：1579504757690004600  时间转换为微秒，
   



Q：看过go-micro源码么，返回码怎么用?redis命令怎么保证原子性？进程间怎么通信？
   
   对共享数据LocalSalesVolume的操作时要使用锁来实现的，但是因为本地扣库存和统一扣库存是一个原子操作，所以在最上层使用channel来实现。统一扣库存操作redis，因为redis是单线程的，而我们要实现从中取数据
   ，写数据并计算一系列步骤，我们要配合lua脚本打包命令，保证操作的原子性
   const LuaScript = `
		local ticket_key = KEYS[1]
		local ticket_total_key = ARGV[1]
		local ticket_sold_key = ARGV[2]
		local ticket_total_nums = tonumber(redis.call('HGET', ticket_key, ticket_total_key))
		local ticket_sold_nums = tonumber(redis.call('HGET', ticket_key, ticket_sold_key))

		--查看是否还有余额，增加订单数量，返回结果值
		if(ticket_total_nums >= ticket_sold_nums) then
				return redis.call('HINCRBY', ticket_key, ticket_sold_key, 1)
		end
		return 0
    `

    lua := redis.NewScript(1, LuaScript)
    result, err := redis.Int(lua.Do(conn, RemoteSpikeKeys.SpikeOrderHashKey, RemoteSpikeKeys.TotalInventoryKey,RemoteSpikeKeys.QuantityOfOrderKey))

    if err != nil {
    	return false
    }






Q：如果通道关闭了，怎么让读请求，写请求退出，这个问题就是怎么优雅地关闭通道？

   关闭通道的原则：不要在消费端关闭信道，不要在有多个并行的生产者时对信道执行关闭操作。也就是说应该只在[唯一的或者最后唯一剩下]的生产者协程中关闭信道，来通知消费者已经没有值可以继续读了。
                   只要坚持这个原则，就可以确保向一个已经关闭的信道发送数据的情况不可能发生。

   记住，关闭的通道依旧可以读，读取的结果为空，写入报错，关闭一个已经关闭的通道也会报错。这里提供一个简单方案，定义一个结构体，里面包括通道，关闭标志，关闭之前读取标志是否已经关闭

   https://studygolang.com/articles/20786?fr=sidebar
   https://www.jianshu.com/p/d24dfbb33781





Q：如果将nil转换给结构体指针，然后通过定义 var p People = 空指针结构体，这时候 p == nil成立么，这个是空接口，非空接口问题？
   https://blog.csdn.net/JackLiu16/article/details/92440421




Q：接收器是值类型，指针类型?


   这两个区别应该是一个可以改结构体属性，一个不可以改吧
   https://studygolang.com/articles/2670


Q：go语言里面的接口，多态，指针，鸭子类型=ducktyping
   
   接口的定义与理解：

    接口是一个自定义类型，它是一组方法的集合。从定义上来看，接口有两个特点。第一，接口本质是一种自定义类型，因此不要将golang中的接口简单理解为C++/Java中的接口，后者仅用于声明方法签名。第二，
    接口是一种特殊的自定义类型，其中没有数据成员，只有方法（也可以为空）。

    接口是完全抽象的，因此不能将其实例化。然而，可以创建一个其类型为接口的变量，它可以被赋值为任何满足该接口类型的实际类型的值。接口的重要特性是：

   （1）只要某个类型实现了接口要的方法，那么我们就说该类型实现了此接口。该类型的值可以赋给该接口的变量;

   （2）作为1的推论，任何类型的值都可以赋值给空接口interface{}

    注意：这只是golang中接口的特性，为非所有类型的特性（接口是一种特殊的类型）。

    接口的特性是golang支持鸭子类型的基础，即“如果它走起来像鸭子，叫起来像鸭子（实现了接口要的方法），它就是一只鸭子（可以被赋值给接口的值）”。凭借接口机制和鸭子类型，golang提供了一种游离于
    类、继承、模板之外的更加灵活强大的选择

   在golang中，值接收者和指针接收者的方法集是不同的。只是golang会智能地解引用和取引用，使得二者的方法集看上去是一样的。但是，在调用exchangeThese时，就凸显出二者的不同了。


Q：GRPC, RPC【remote control protocol】的区别，GRPC的工作流程，GRPC底层实现原理
   RPC，是一种request-response模式(http协议也是这种)的远程过程调用协议，调用远程应用如同调用本机应用一样简单，基于client-server的一种网络模型，
   是进程通信[跨主机间进程通讯]的一种技术。
   1，客户端链接服务端
   2，构造用于远程调用请求的message(xml, json等便于格式化便于传输的数据结构)
   3，客户端发送远程调用请求到远程服务端，并且同步等待服务端请求
   4，服务端接收远程调用请求
   5，处理远程调用请求
   6，服务端构造应答的message
   7，客户端接收远程调用的应答完成远程调用

   GRPC是一个高性能、开源和通用的 RPC 框架，面向移动和 HTTP/2 设计。gRPC 默认使用 protocol buffers，这是 Google 开源的一套成熟的结构数据序列化机制
  （当然也可以使用其他数据格式如 JSON-rpc, xml-rpc）.gRPC是rpc的一个升级版，gRPC与protobuf结合使用，也可以使用json进行数据交互.
   GRPC工作流程：
   1，使用protobuf定义服务，服务请求，服务应答
   2，使用protobuf对应的编译工具把protobuf语法编译成对应语言格式，例如golang语言
   3，服务端实现服务，启动GRPC服务等客户端调用
   4，通讯过程：
      1.客户端连接远程服务
      2.客户端构造远程调用的protobuf请求
      3.客户端执行远程调用，并且同步等待调用结果
      4.服务端接收到请求进行处理
      5.服务端构造远程调用的protobuf应答
      6.服务端把结果返回给调用者
      7.客户端接收应答


Q：Restful,REST,
   API:程序或者系统的接口。例如，系统函数只提供了函数名，参数等，这个就称为API。RESTful API是目前比较成熟的一套互联网应用程序的API设计理论。

   网络应用程序，分为前端和后端两个部分。当前的发展趋势，就是前端设备层出不穷（手机、平板、桌面电脑、其他专用设备......）。因此，必须有一种统一的机制，方便不同的前端设备与后端进行通信。这导致API构架的流行，甚至出现"APIFirst"的设计思想。RESTful API是目前比较成熟的一套互联网应用程序的API设计理论。

   REST（Representational State Transfer）表述性状态转换，REST指的是一组架构约束条件和原则。 如果一个架构符合REST的约束条件和原则，我们就称它为RESTful架构。
   REST本身并没有创造新的技术、组件或服务，而隐藏在RESTful背后的理念就是使用Web的现有特征和能力，更好地使用现有Web标准中的一些准则和约束。虽然REST本身受Web技术的影响很深，但是理论上REST架构风格并不是绑定在HTTP上，只不过目前HTTP是唯一与REST相关的实例。

Resource（资源） ：·对象的单个实例。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。
		   用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。

集合：对象的集合。 例如，动物。

第三方：使用我们接口的开发者

表现层（Representation）

"资源"是一种信息实体，它可以有多种外在表现形式。我们把"资源"具体呈现出来的形式，叫做它的"表现层"（Representation）。

状态转化（State Transfer）

访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生"状态转化"（State Transfer）。而这种转化是建立在表现层之上的，所以就是"表现层状态转化"。

客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。

比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的".html"后缀名是不必要的，因为这个后缀名表示格式，属于"表现层"范畴，

而URI应该只代表"资源"的位置。它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对"表现层"的描述。


综合上面的解释，我们总结一下什么是RESTful架构：

（1）每一个URI代表一种资源；

（2）客户端和服务器之间，传递这种资源的某种表现层；

（3）客户端通过四个HTTP动词，对服务器端资源进行操作，实现"表现层状态转化"。


URL设计：

客户端发出的数据操作指令都为 动词+宾语。根据 HTTP 规范，动词一律大写
eg. GET /article

宾语必须是名词，宾语就是 API 的 URL，是 HTTP 动词作用的对象。它应该是名词，不能是动词


动词的override：
有些客户端只能用GET，POST方法，服务器必须通过POST模拟其他三个方法：
PUT delete PATCH
客户端打出的HTTP请求，要加上这个属性# X-HTTP-Method-Override 告诉服务器用哪一个动词覆盖POST

POST /api/Person/4 HTTP/1.1
X-HTTP-Method-Override: PUT  //指定这个的请求方法是PUT，而不是POST


Q：微服务里protobuf，
   protobuf是一种高效的结构化数据存储格式，可用于通讯协议，数据存储等领域。它比XML更加小巧轻便，快速简单。可以自己定义数据结构，然后快速用代码生成器生成的代码来读写这个数据结构，甚至可以在
   无需重新部署程序的情况下更新数据结构，只需要使用protobuf对数据结构进行一次扫描，即可以利用各种不同语言或者各种不同数据流中对自己的结构化数据轻松读写。


Q：Nginx是什么，怎么理解的Nginx，底层原理？
   该模块为c语言编写的，免费的，开源，高性能服务器，本身是一个静态资源服务器。主要功能是请求的处理与转发，面试的时候有人说用来部署。 
   可以做http服务器[包括动静分离]，正向，反向代理服务器，邮件代理服务器，普通的tcp/udp服务器，负载均衡等

   正向代理：
	一般情况下，如果没有特别说明，代理技术默认说的是正向代理技术。关于正向代理的概念如下：
	正向代理(forward)是一个位于客户端【用户A】和原始服务器(origin server)【服务器B】之间的服务器【代理服务器Z】，为了从原始服务器取内容，用户A 向代理服务器 Z 发送一个请求并指定目标（服务器B）， 
        然后代理服务器 Z 向服务器 B 转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。
	从上面的概念中，可以看出，所谓的正向代理就是代理服务器替代访问方【用户A】去访问目标服务器【服务器B】。
	eg. 为在防火墙里的局域网用户提供访问Internet的途径。学校的局域网，单位局域网访问外网。


   反向代理：这里有一个单词：upstream，这个代表反向。
	反向代理正好与正向代理相反，对于客户端而言代理服务器就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端。指代理服务器接收来自Internet客户端的链接请求，然后将请求转发给内网里的原始服务器，并且将原始服务器的结果转发给客户端。
	eg.淘宝用户选择支付宝支付，支付请求发给代理服务器，代理服务器转发给集群里的原始服务器或者内网里的原始服务器，原始服务器将结果转发给代理服务器，然后代理服务器转发给淘宝用户。 


Q：负载均衡的几种策略模式，这几种的区别以及如何抉择集中模式？除了Nginx可以配置外，还有其他的框架或者方案么？
   负载均衡：英文名称为 Load Balance，是指建立在现有网络结构之上，并提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。其原理就是数 据流量分摊到多个服务器上执行，减轻每台服务器的压力，多台服务器共同完成工作任务，从而提高了数据的吞吐量。就是一个 web 服务器解决不了的问题可以通过多个 web 服务器来平均分担压力来解决，并发过来的请求被平均分配到多个后台 web 服务器来处理，这样压力就被分解开来。

   设备的角度来看，负载均衡服务器分为两种：
   一种是通过硬件实现的负载均衡服务器，简称硬负载例如：f5。另一种是通过软件来实现的负载均衡，简称软负载，例如 apache 和 nginx。硬负载和软负载相比，
   前者作用的网络层次比较多，可以作用到 socket 接口的数据链路层对发出的请求进行分组转发但是价格成本比较贵，而软负载作用的层次在 http 协议层之上可以对 http 请求进行分组转发并且因为是开源的所以几乎是0 成本，并且阿里巴巴，京东等电商网站使用的都是 Nginx 服务器。包括我们耳熟能详的LVS，Tengine（阿里对Nginx进行的改造）

   从负载均衡的技术来看，分为服务端负载均衡和客户端负载均衡：
   服务端负载均衡：当我们访问一个服务，请求会先到另外一台服务器，然后这台服务器会把请求分发到提供这个服务的服务器，当然如果只有一台服务器，那好说，直接把请求给那一台服务器就可以了，但是如果有多台服务器呢？这时候，就会根据一定的算法选择其中一台服务器。
   客户端负载均衡：客户端服务均衡的概念貌似是有了服务治理才产生的，简单的来说，就是在一台服务器上维护着所有服务的ip，名称等信息，当我们在代码中访问一个服务，通过一个组件访问的，这个组件会从那台服务器上取到所有提供这个服务的服务器的信息，然后通过一定的算法，选择一台服务器进行请求。

   nginx的http请求怎么到 beego了，ngx_http_proxy_module模块代理请求？

 http -> server


server  {
	listen		80;
	server_name	localhost;

	charset		utf8;

	#客户端发起了一个请求，
	location / {
		# 进行数据转发，指定一个代理地址
		# http://固定的前缀
		# test.com -> 字节编得一个名字
		proxy_pass http://test.com;
	}

	location /hello/ {
		# 进行数据转发，指定一个代理地址
		# http://固定的前缀
		# test.com -> 字节编得一个名字
		proxy_pass http://test.com;   //这个名称必须与upstream名称保持一致
	}
}

# 转发处理 实现负载均衡通过upstream模块实现
upstream test.com 
{
	server 192.168.1.131:80 weight = 1;
	server 192.168.1.136:80 weight = 1;
}


func main() {

		service := micro.NewService(
			micro.Name("go.micro.srv.srv"),
			micro.Version("lstest"),
		)

		//initialize service
		service.Init()
}


负载均衡的算法又分为 随机，轮询，哈希，最小压力，当然可能还会加上权重的概念，负载均衡的算法就是本文的重点了。


随机    ：随机就是没有规律的，随便从负载中获得一台，又分为完全随机和加权随机：

完全随机：有的服务器经常被获得到，有的服务器获得的次数比较少，但是当有充足的请求次数，就会越来越平均，这正是随机数的一个特性。完全随机是最简单的负载均衡算法了，
          缺点比较明显，因为服务器有好有坏，处理能力是不同的，我们希望性能好的服务器多处理些请求，性能差的服务器少处理一些请求，所以就有了加权随机。

加权随机：虽然还是采用的随机算法，但是为每台服务器设置了权重，权重大的服务器获得的概率大一些，权重小的服务器获得的概率小一些。

          关于加权随机的算法，有两种实现方式：
          一种是网上流传的，代码比较简单：构建一个服务器的List，如果A服务器的权重是2，那么往List里面Add两次A服务器，如果B服务器的权重是7，那么我往List里面Add7次B服务器，以此类推，然后我再生成一个随           机数，随机数的上限就是权重的总和，也就是List的Size。这样权重越大的，被选中的概率当然越高。理解为一个列表存了两个2，8个7
          然后随机随机数范围为 0 -- len(列表)，随机数为多少，获取列表[随机数]对应的服务器。


          另一种方法：如果A服务器的权重是2，B服务器的权重是7，C服务器的权重是1：这里的权重值为每个范围的长度。
          如果我生成的随机数是1[属于 1-2]，那么落到A服务器，因为1<=2（A服务器的权重）
          如果我生成的随机数是5[属于 3-9]，那么落到B服务器，因为5>2（A服务器的权重），5-2（A服务器的权重）=3，3<7（B服务器的权重）
          如果我生成的随机数是10[属于 10-OO]，那么落到C服务器，因为10>2（A服务器的权重），10-2（A服务器的权重）=8，8>7（B服务器的权重），8-7（B服务器的权重）=1，
          1<=1（C服务器的权重）

          范围图：这个加权轮询就是每个服务器有一个加权值，都有一个范围，每次的随机数从第一个加权值开始减，余数小于哪一个加权值就用这个服务器。
	  --|-------|-
	 1-2|  3-9  |10


轮询：

轮询分为三种，完全轮询，加权轮询，平滑加权轮询

完全轮询：过程类似于完全随机

加权轮询：过程类似于加权随机

平滑加权轮询：每个服务器有一个固定权重，例如：A权重值：6，B权重值：1，C权重值：1。
              第一次访问，非固定权重为6,1,1.第一次随机数如果为5,应该落在A服务器。此时，非固定权重变为为0,1,1。【6-8=-2，落在那个服务器，该服务非固定权重变为：
                服务器固定权重 - 权重和】
              接下来每次访问通过负载均衡，此时非固定权重在上一次已经有修改，用非固定权重列表 = 固定权重列表 + 非固定权重列表。产生的随机数属于非固定权重哪一个范围，
              就落在那个服务器，然后按照上述步骤继续更新非固定权重。
              依次访问通过负载均衡，不停更新非固定权重，每次通过获取的随机数，依次从第一个权重轮询，看随机数处于什么范围，选择落在什么服务器。这样保证了随机。

这就是平滑加权轮询，巧妙的利用了巧妙算法，既有轮询的效果，又避免了某台服务器压力突然升高，不可谓不妙。


哈希：

负载均衡算法中的哈希算法，就是根据某个值生成一个哈希值，然后对应到某台服务器上去，当然可以根据用户，也可以根据请求参数，或者根据其他，想怎么来就怎么来。如果根据用户，就比较巧妙的解决了负载均衡下Session共享的问题，用户小明走的永远是A服务器，用户小笨永远走的是B服务器。

那么如何用代码实现呢，这里又需要引出一个新的概念：哈希环。

 __A__a1__b1__
|             |     在哈希环里面分布着很多个节点，A，B。虚拟节点 a1，b1。例如根据ip进行哈希，将哈希放到环里各个节点。有一个请求根据某一个值进行哈希，如果计算出的哈
|__B__________|     希落到了A-a1,or a1_b1之间，根据顺时针原则，落在了服务器a1, b1，如果是虚拟节点，会指向虚拟节点映射的服务器，如果是正常节点直接命中。


最小压力：

所以的最小压力负载均衡算法就是 选择一台当前最“悠闲”的服务器，如果A服务器有100个请求，B服务器有5个请求，而C服务器只有3个请求，那么毫无疑问会选择C服务器，这种负载均衡算法是比较科学的。但是遗憾的在当前的场景下无法模拟出来“原汁原味”的最小压力负载均衡算法的。


当然在实际的负载均衡下，可能会将多个负载均衡算法合在一起实现，比如先根据最小压力算法，当有几台服务器的压力一样小的时候，再根据权重取出一台服务器，如果权重也一样，再随机取一台，等等。


Q：分布式计算，分布式存储，分布式缓存，实现各分布式的锁，去中心化问题。
   分布式计算：把相同的处理逻辑并行的运行在多台计算机上。分布式计算是利用互联网上的计算机的中央处理器的闲置处理能力来解决大型计算问题的一种计算科学。

分布式系统的CAP理论：

一致性(Consistency，C)：所有节点访问同一份最新的数据副本。在分布式系统中的所有数据备份，在同一时刻是否是同样的值。

可用性(Availability，A)：对数据更新具备高可用性。在集群中一部分结点故障后，集群整体是否还能响应客户端的读写请求。

分区容忍性(Partition tolerance，P)：当集群中的某些结点无法联系时仍能正常提供服务。


CAP中的选择

在设计分布式应用系统时，3大要素最多只能同时实现2点，不可能三者兼顾。

    选择分区容忍性和一致性（CP）
    即使结点故障，操作也必须一致，并能顺利完成。最好的方法就是将所有数据放到同一个结点中。但这种方式显然不满足可用性。如：BigTable、HBase

    选择分区容忍性和可用性（AP）
    满足可用，就说明数据必须要在不同结点中有副本。如果还必须保证在产生分区的时候仍然可以完成操作，那么操作就无法保证一致性。如：Dynamo、SimpleDB

    选择可用性和一致性（CA）
    有一致性和可用性的系统通常可扩展性能不高，不具有分区容错性，如传统的关系型数据库。


    分布式存储：https://baijiahao.baidu.com/s?id=1623275115956708740&wfr=spider&for=pc，https://blog.csdn.net/qq_41534566/article/details/82900083


    分布式缓存： redis？在设计缓存一致性更新模式时，无非就是Cache Aside、Read/Write Through和Write Behind这三大种模式。


Q：分布式系统如何防止突然增大的流量。如何实现一个动态的并发量，根据用户访问量的大小，这里应该涉及到分流这些？
   https://www.jianshu.com/p/fd9b2b90457e


Q：FastDFS的工作原理，介绍下FastDFS分布式方案，FastDFS同类的框架有了解么，怎么部署的？
   https://blog.csdn.net/xiamoyanyulrq/article/details/81273745


Q：mysql的数据类型有哪几种，datetime,timestamp的区别？
   mysql的数据类型三大类，数字型，数值型，时间类型。


   其中，时间类型分为以下几种：


   Date
名称 	        解释
显示格式 	YYYY-MM-DD
显示范围 	1601-01-01 到 9999-01-01
应用场景 	当业务需求中只需要精确到天时，可以用这个时间格式
后台取值 	@JSONField(format=”yyyy-MM-dd”)


   DateTime，占用8个字节，实际存储的值与显示的值一样，可以为空值或者自定义值，系统不会自动修改其值，可以用now()变量来自动插入当前系统时间
名称 	        解释
显示格式 	YYYY-MM-DD HH:mm:ss 
显示范围 	1601-01-01 00:00:00 到 9999-12-31 23:59:59
应用场景 	当业务需求中需要精确到秒时，可以用这个时间格式，涉及到时区就用不了这种类型
后台取值 	@JSONField(format=”yyyy-MM-dd HH:mm:ss”)


   TimeStamp，只占用4个字节，值以UTC格式保存，允许为空值，系统会自动按照当前系统时间插入表中，
名称 	        解释
显示格式 	YYYY-MM-DD HH:mm:ss
显示范围 	1970-01-01 00:00:00 到 2037-12-31 23:59:59
应用场景 	当业务需求中需要精确到秒或者毫秒时，或者该系统用于不同时区，可以用这个时间格式
后台取值 	@JSONField(format=”yyyy-MM-dd HH:mm:ss:SSS”)（这里只会精确到秒）


   Time
名称 	        解释
显示格式 	HH:mm:ss
显示范围 	00:00:00 到 23:59:59
应用场景 	当业务需求中只需要每天的时间，可以用这个时间格式
后台取值 	@JSONField(format=”HH:mm”)（这里是只精确到分，数据库存的也就是只精确到分，比如 09:36:00，如果需要精确到秒。需要使用@JSONField(format=”HH:mm:ss”)）


Q：mysql事务是什么？mysql怎么进行优化[一个表有几千万笔数据，怎么优化查询速度]，索引类型有哪些，怎么使用了？
   四个特性，记住我的项目里面在下单过程有开始事务，开启事务--下单减库存--成功了再关闭事务


Q：go 切片问题。
   怎么删除/增加一个区切片中的某一个元素，不能新建立切片？

以下两种方法是新建切片来存储新切片结果，切记，这里会改变原来切片的值
删除元素

    l := []int{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
    newL := append(l[0:5], l[6:]...)

增加元素

    l := []int{0, 1, 2, 3, 4, 6, 7, 8, 9}
    newL := append(l[0:5], append([]int{5}, l[5:]...)...)

这两种是不建立新切片
   l = append(l[0:5], l[6:]...)
   l = append(l[0:5], append([]int{5}, l[5:]...)...)

   怎么实现切片的反转？
   func reverse(s []int) []int {
	for i, j := 0, len(s)-1; i < j; i, j = i+1, j-1 {
		s[i], s[j] = s[j], s[i]
	}
	return s
   }


Q：go map问题。一个map在不加锁的情况下，多个写成对map同时写入数据，最严重到什么程度？map为什么是无序的？
   因为map不是线程安全的，导致数据不一致，引发panic异常，这时候需要用一个读写锁。编程的时候需要考虑go程或者其他线程对同一个map有没有多个读写。
   map为什么遍历是无序的，https://blog.csdn.net/QQ1528884535/article/details/101443377，暂时说因为源码这样设计的，for range map在开始遍历的时候，就做了随机播种，然后才继续遍历


Q：go interface问题
   函数的参数定义为interface类型可以接收多种类型的数据，利用类型断言。

   //类型断言  是基于接口类型数据的转换
	  Go当中自然也有相应的方法来判断类型，即Comma-ok断言：

	写法为 if value, ok := em.(T); ok { }  //  em必须为initerface类型才可以进行类型断言
	em代表要判断的变量 

	T代表被判断的类型

	value代表返回的值，就是类型

	ok代表是否为该类型


Q：go语言反射的原理
   Golang语言实现了反射，反射机制就是在运行时动态的调用对象的方法和属性，官方自带的reflect包就是反射相关的，只要包含这个包就可以使用。多插一句，Golang的gRPC也是通过反射实现的。
   反射就是用来检测存储在接口变量内部(值value；类型concrete type) pair对的一种机制。反射的基本功能：reflect.ValueOf() 和 reflect.TypeOf()


    变量包括（type, value）两部分，理解这一点就知道为什么nil != nil了

    type 包括 static type和concrete type. 简单来说 static type是你在编码是看见的类型(如int、string)，concrete type是runtime系统看见的类型

    类型断言能否成功，取决于变量的concrete type，而不是static type. 因此，一个 reader变量如果它的concrete type也实现了write方法的话，它也可以被类型断言为writer.


   反射就是建立在类型之上的，Golang的指定类型的变量的类型是静态的（也就是指定int、string这些的变量，它的type是static type），在创建变量的时候就已经确定，
   反射主要与Golang的interface类型相关（它的type是concrete type），只有interface类型才有反射一说。

   在Golang的实现中，每个interface变量都有一个对应pair，pair中记录了实际变量的值和类型: (value, type)。value是实际变量值，type是实际变量的类型。
   一个interface{}类型的变量包含了2个指针，一个指针指向值的类型【对应concrete type】，另外一个指针指向实际的值【对应value】。

   反射可以大大提高程序的灵活性，使得interface{}有更大的发挥余地
        反射必须结合interface才玩得转
        变量的type要是concrete type的（也就是interface变量）才有反射一说

   反射可以将“接口类型变量”转换为“反射类型对象”
        反射使用 TypeOf 和 ValueOf 函数从接口中获取目标对象信息

   反射可以将“反射类型对象”转换为“接口类型变量
        reflect.value.Interface().(已知的类型)
        遍历reflect.Type的Field获取其Field

   反射可以修改反射类型对象，但是其值必须是“addressable”
        想要利用反射修改对象状态，前提是 interface.data 是 settable,即 pointer-interface

   通过反射可以“动态”调用方法

   因为Golang本身不支持模板，因此在以往需要使用模板的场景下往往就需要使用反射(reflect)来实现


Q：beego的一些总结？

   结构体解析：

   type Controller struct {
	// context data 上下文数据
	Ctx            *context.Context
	Data map[interface{}]interface{}
 
	// route controller info  路由控制信息

	controllerName string
	actionName     string
	methodMapping  map[string]func() //method:routertree
	AppController  interface{}
 
	// template data  模板（View）数据
	TplName        string
	ViewPath       string
	Layout         string
	LayoutSections map[string]string // the key is the section name and the value is the template name
	TplPrefix      string
	TplExt         string
	EnableRender   bool
 
	// xsrf data  跨站域请求伪造 数据
	_xsrfToken   string
	XSRFExpire   int
	EnableXSRF   bool
 
	// session
	CruSession session.Store
   }
   

   beego.Controller有的一些方法，我们项目里写的controller都是组合了beego.Controller，然后重写这些对应的方法，明白么，override？

   type ControllerInterface interface {
 
	Init(ct *context.Context, controllerName, actionName string, app interface{})
	/*
	主要用来初始化Context、 Controller 名称，模板名，初始化模板参数的容器 Data, app 可以用来执行子类的方法
	*/

	Prepare()
	/*
	这个函数主要是为了用户扩展用的，这个函数会在下面定义的这些 Method 方法之前执行，用户可以重写这个函数实现类似用户验证之类。
	*/
	
	Get()    //如果用户请求的 HTTP Method 是 GET，那么就执行该函数
	Post()   //如果用户请求的 HTTP Method 是 POST，那么就执行该函数
	Delete() //如果用户请求的 HTTP Method 是 DELETE，那么就执行该函数
	Put()    //如果用户请求的 HTTP Method 是 PUT，那么就执行该函数
	Head()   //如果用户请求的 HTTP Method 是 HEAD，那么就执行该函数
	Patch()  //如果用户请求的 HTTP Method 是 PATCH，那么就执行该函数
	Options()//如果用户请求的 HTTP Method 是 OPTIONS，那么就执行该函数

	Finish()
	/*
	这个函数是在执行完相应的 HTTP Method 方法之后执行的，默认是空，用户可以在子 struct 中重写这个函数，执行例如数据库关闭，清理数据之类的工作。
	*/

	Render() error	//这个函数主要用来实现渲染模板，如果 beego.AutoRender 为 true 的情况下才会执行。
	XSRFToken() string  //跨站请求伪造验证，这个不太明白
	CheckXSRFCookie() bool
	HandlerFunc(fn string) bool
	URLMapping()  
   }


   beego框架的路由
   以post,get为例，写一个beego框架的路由，具备反射基础

   导入router包：import "beegoframework/routers"

   初始化：func int(){	//init中
		fmt.Println("router init")
		//写路由，指定路由，并且制定处理请求的方法
		beego.Router("/login", &controllers.LoginController{},"get:LoginGet;post:LoginPost")
		beego.Router("/registerUser", &controllers.RegisterController{},"get:RegisterUserGet;post:RegisterUserPost")
	}
    //这些方法通常在实现了ControllerInterface接口的结构体中
    type ControllerInfo struct {
		pattern		string				//存储url
		controllerType	reflect.Type			//存储控制器具体类型(即ControllerInterface)
		methods		map[string]string		//存储请求方法
		initialize	func() ControllerInterface	//初始化方法
	}


	在Router方法中，通过接收LoginController结构体对象，通过反射相关api获取结构体参数类型，目前获取的是LoginController结构体。方法存储在methods切片里，最终结果为GET->loginGet, POST->LoginPost

	创建ControllerInfo结构体，在ControllerInfo中定义多个字段，用于存储数据

	type Controller struct {
		ControllerInterface
	}


	type ControllerInterface interface {
		Init(ct *context.Context, controllerName, actionName string, app interface{})
		Get()
		Post()
	}


	在Router方法中，将url存储到pattern，控制器类型存储在controllerType中，将请求方式以及请求方法放在methods中，初始化LoginController的方法放置在initialize字段中。


	在beego.Run中调用http.ListenAndServe(":8080",HttpTest{})方法，代表在8080端口开启服务。HttpTest{}结构体中实现Handler接口中ServeHTTP(rw http.ResponseWriter, r *http.Request)方法，
	用于重写浏览器发过来的请求。

	
	在ServeHTTP方法调用的时候，即接收到请求的时候，通过r，也就是*http.Request获取浏览器发送请求类型以及请求链接地址，即调用r.Method以及r.URL.Path。
	

	获取本次请求的类型【是get?post?】通过类型匹配请求的方法，如果匹配的方法名称是LoginGet，则调用LoginController中的LoginGet方法


   ORM：即对象关系映射（Object Relational Mapping，简称ORM）
	//数据库的表（table） --> 结构体（Struct）
	//记录（record，行数据） --> 结构体实例对象（object）
	//字段（field） --> 对象的属性（attribute）
        每次使用orm需要这些：
        o := orm.NewOrm()   
        var user models.User  //这两个是实例化一笔记录了？
        user.属性 = value     //这样可以直接给这笔记录赋值

        _,err :=o.Insert(&user) //直接插入一笔记录
        
        o.Update(&user)         //更新记录

        err := o.Read(&user)    //这个应该是按照主键读取记录
        err := o.Read(&user,"Name")  //这个应该是按照其他字段读取记录

	//ORM可关联其他表进行jopin查询
        o.QueryTable("OrderInfo").RelatedSel("User").Filter("User__Id",user.Id).All(&orderInfos)

	使用ORM需要进行这些操作
        func (this * ORMdemoController) Get(){
    		//注册数据驱动
    		orm.RegisterDriver("mysql", orm.DRMySQL) // mysql / sqlite3 / postgres 这三种是beego默认已经注册过的，所以可以无需设置

    		//注册数据库 ORM 必须注册一个别名为 default 的数据库，作为默认使用
    		orm.RegisterDataBase("default", "mysql", "root:yangjie@tcp(127.0.0.1:3306)/db_user?charset=utf8")
				*参数1 数据库的别名，ORM 必须注册一个别名为 default 的数据库，作为默认使用。
				*参数2 数据库驱动
				*参数3 对应的连接字符串，root和yangjie数据库链接账号和密码 ，tcp(127.0.0.1:3306)数据库链接地址，db_user是使用的库
				*参数4(可选) 设置最大空闲连接
				*参数5(可选) 设置最大数据库连接 (go >= 1.2)

    		//注册模型，就是在models里面定义的结构体
    		orm.RegisterModel(new(UserTable))

    		//自动创建表 参数二为是否开启创建表   参数三是否更新表
    		orm.RunSyncdb("default", true, true)
				*参数一 数据库别名，一般为default；
				*参数二 为ture时自动帮我们创建表；
				*参数三 是否更新表

    		this.Ctx.WriteString("表创建成功")
	}


        model里面的结构体在建立的时候需要使用StructTag



Q：cookie与session的区别？
   Cookie
   1.Http是无状态的协议，服务器不能记录浏览器的访问状态，也就是服务器不能区分两次请求是否是来自同一个客户端。

   2.Cookie实际上是服务器保存在浏览器上的一段信息，浏览器有了Cookie之后，每次向服务器发送请求都会带着该信息进行访问，服务器在收到请求之后，就可以通过该信息进行处理。

   3.Cookie由服务器创建，并发给浏览器，最终由浏览器保存。

   Cookie的用途
    保持用户登陆状态
    电商网站的购物车

   通过GetCookie判断浏览器是否有Cookie ，如果没有就使用SetCookie设置，如果有就获取并输出。

   if this.Ctx.GetCookie("user") ==""{  
	this.Ctx.SetCookie("user","admin")  //  SetCookie 参数一为cookie的名称，参数二为cookie的值。设置Cookie是不能存在中文的，如果存在中文虽然可以设置成功，但是不能取出。 
	this.Ctx.WriteString("Cookie设置成功")
   }else{
	user:= this.Ctx.GetCookie("user")
	this.Ctx.WriteString("user="+user)
   }


   Session

   Session是一段保存在服务器上的信息，当客户端第一次访问服务器时创建Session，同时也会创建一个名为beegosessionID,值为创建的Session的id的Cookie。
   这个beegosessionID对应服务器中的一个Session对象，通过它就可以获取到保存用户信息的Session。
   Session 是服务器创建的，也是保存在服务器上。

   if this.GetSession("user") == nil {
	this.SetSession("user", "admin")
	this.Ctx.WriteString("Session设置成功!")
   }else {
	//获取session
	username := this.GetSession("user")
	fmt.Println("sessionid = ", this.CruSession.SessionID())
	this.Ctx.WriteString("user = " + username.(string))
   }


Q：goroutine与线程的区别，goroutine有什么好处？高并发线程有什么问题？GMP模型。
   https://www.jianshu.com/p/abe79d86ff27


Q：如果一个goroutine运算量很大一直占用CPU，其他的go程只能等待，这个怎么解决，听说这个问题在1.5版本之前是个bug
   https://www.codercto.com/a/81630.html


Q：go语言GC，垃圾回收机制
   首先：程序创建的对象都标记为白色。

   gc开始：扫描所有可到达的对象，标记为灰色

   从灰色对象中找到其引用对象标记为灰色，把灰色对象本身标记为黑色

   监视对象中的内存修改，并持续上一步的操作，直到灰色标记的对象不存在

   此时，gc回收白色对象。

   最后，将所有黑色对象变为白色，并重复以上所有过程。


gc和用户逻辑如何并行操作？

标记-清除(mark and sweep)算法的STW(stop the world)操作，就是runtime把所有的线程全部冻结掉，所有的线程全部冻结意味着用户逻辑是暂停的。这样所有的对象都不会被修改了，这时候去扫描是绝对安全的。

Go如何减短这个过程呢？标记-清除(mark and sweep)算法包含两部分逻辑：标记和清除。

我们知道Golang三色标记法中最后只剩下的黑白两种对象，黑色对象是程序恢复后接着使用的对象，如果不碰触黑色对象，只清除白色的对象，肯定不会影响程序逻辑。所以：清除操作和用户逻辑可以并发。

标记操作和用户逻辑也是并发的，用户逻辑会时常生成对象或者改变对象的引用，那么标记和用户逻辑如何并发呢？


Golang引入了写屏障这个机制。

写屏障：该屏障之前的写操作和之后的写操作相比，先被系统其它组件感知。

通俗的讲：就是在gc跑的过程中，可以监控对象的内存修改，并对对象进行重新标记。(实际上也是超短暂的stw，然后对对象进行标记)

在上述情况中，新生成的对象，一律都标位灰色！


那么，灰色或者黑色对象的引用改为白色对象的时候，Golang是该如何操作的？看如下图，一个黑色对象引用了曾经标记的白色对象。这时候，写屏障机制被触发，向GC发送信号，GC重新扫描对象并标位灰色。


因此，gc一旦开始，无论是创建对象还是对象的引用改变，都会先变为灰色。


Q：对称加密，非对称加密的原理，区别，以及PKI。

常用的两种加密方式

- 对称加密
	- 秘钥: 加密解密使用的是同一个秘钥, 秘钥有一个
	- 特点
		- 双方向保证机密性
		- 加密效率高, 适合加密大数据, 大文件
		- 加密强度不高, 相对于非对称加密
- 非对称加密
	- 秘钥: 加密解密使用的不同的秘钥, 秘钥有两个, 需要使用秘钥生成算法, 得到密钥对
		- 公钥 - 可以公开的秘钥:
		- 公钥加密数据, 解密需要使用私钥
		- 私钥 - 需要妥善保管的秘钥, 知道的人越少越好:
		- 私钥加密, 公钥解密
	- 特点:
		- 数据的机密性只能单方向保证
		- 加密效率低, 适合加密少量数据
		- 加密强度高, 相对于对称加密


Q：公钥，私钥，数字签名跟公钥，私钥的关系

公钥加密
	1. 将公钥文件中的公钥读出, 得到使用pem编码的字符串
	2. 将得到的字符串解码
	3. 使用x509将编码之后的公钥解析出来
	4. 使用得到的公钥通过rsa进行数据加密


私钥解密
	1. 将私钥文件中的私钥读出, 得到使用pem编码的字符串
	2. 将得到的字符串解码
	3. 使用x509将编码之后的私钥解析出来
	4. 使用得到的私钥通过rsa进行数据解密


单向散列函数（one-waynction）：
有一个输人和一个输出，其中输人称为消息（message），输出称为散列值（hashvalue）。
单向散列函数可以根据消息的内容计算出散列值，而散列值就可以被用来检查消息的完整性。
这里的消息不一定是人类能够读懂的文字，也可以是图像文件或者声音文件。单向散列函数不需要知道消息实际代表的含义。
无论任何消息，单向散列函数都会将它作为单纯的比特序列来处理，即根据比特序列计算出散列值。散列值的长度和消息的长度无关。
无论消息是1比特，还是100MB，甚至是IOOGB，单向散列函数都会计算出固定长度的散列值。以SHA-I单向散列函数为例，
它所计算出的散列值的长度永远是160比特（20字节）。

两个不同的输有不同的散列值。这个也可以理解为哈希。


消息认证码：
消息认证码（message authentication code）是一种确认完整性并进行认证的技术，取三个单词的首字母，简称为MAC。
将“发送者和接收者之间的共享密钥”和“消息，进行混合后计算出的散列值。使用消息认证码可以检测并防止通信过程中的错误、
篡改以及伪装。消息认证码的输入包括任意长度的消息和一个发送者与接收者之间共享的密钥，它可以输出固定长度的数
据，这个数据称为MAC值。根据任意长度的消息输出固定长度的数据，这一点和单向散列函数很类似。但是单向散列函数中计算散列
值时不需要密钥，而消息认证码中则需要使用发送者与接收者之间共享的密钥。消息认证码有很多种实现方法，大家可以暂且这样理解：
消息认证码是一种与密钥相关联的单向散列函数。
消息验证码的使用过程：这个只适用两个人之间验证完整性，通过第三方验证就不一定正确
	1. 发送者Alice与接收者Bob事先共享密钥。
	2. 发送者Alice根据汇款请求消息计算MAC值（使用共享密钥）。
	3. 发送者Alice将汇款请求消息和MAC值两者发送给接收者Bob。
	4. 接收者Bob根据接收到的汇款请求消息计算MAC值（使用共享密钥）。
	5. 接收者Bob将自己计算的MAC值与从Alice处收到的MAC值进行对比。
	6. 如果两个MAC值一致，则接收者Bob就可以断定汇款请求的确来自Alice（认证成功）；如果不一致，则可以断定消息不是来自Alice（认证失败）。

数字签名：
在进行数字签名时也会使用单向散列函数。数字签名是现实社会中的签名（sign）和盖章这样的行为在数字世界中的实现。
使用数字签名可以识别篡改和伪装，还可以防止否认。数字签名的处理过程非常耗时，因此一般不会对整个消息内容直接施加数字签名，\
而是先通过单向散列函数计算出消息的散列值，然后再对这个散列值施加数字签名。
（1）Alice用单向散列函数计算消息的散列值。
（2）Alice用自己的私钥对散列值进行加密。（用私钥加密散列值所得到的密文就是Alice对这条散列值的签名，
由于只有Alice才持有自己的私钥因此, 除了Alice以外，其他人是无法生成相同的签名（密文）的）
（3）Alice将消息和签名发送给Bob。
（4）Bob用Alice的公钥对收到的签名进行解密。（如果收到的签名确实是用Alice的私钥进行加密而得到的密文（签名），那么用
Alice的公钥应该能够正确解密，解密的结果应该等于消息的散列值。如果收到的签名不是用Alice的私钥进行加密而得到的密文，
那么就无法用Alice的公钥正确解密（解密后得到的数据看起来是随机的））
（5）Bob将签名解密后得到的散列值与Alice直接发送的消息的散列值进行对比。（如果两者一致，则签名验证成功；
如果两者不一致，则签名验证失败。）


Q：以淘宝为例，每天几十亿请求量，设计后台的时候包括哪些部分？
   前端页面缓存，CDN处理[用户网络分发]，浏览器缓存。后端redis, memechach，文件缓存，数据库缓存，数据库读写分离，主从复制。


Q：输入域名，怎么知道访问的是 https or http？
   浏览器默认的端口一直都是80端口，也就是说默认情况下它是走的http协议，那么为了让它通过https协议来访网站，服务器实际上做了这些事情。
   1.使用http协议并监听80端口，等待浏览器的访问
   2.在监听的80端口上设置url重定向，指向监听端口为443的https协议的网站
   3.通过这样一种方式就可以实现网站地址的重定向了


Q：浏览器访问url整个流程，重点记住七层协议

用户通过浏览器访问网页，在应用层就是用户的浏览器和服务器的Web App会话，而建立应用层的会话需要依托TCP/IP协议封装与数据传输，具体步骤有：

（1）用户输入URL，其中的domain解析全过程参考【域名解析过程】,解析URL成目标IP地址

（2）浏览器代为封装成符合http格式的Request请求，包含请求首行、请求头和请求体，就是给目标IP发送请求包

（3）Request请求是应用层数据，再由OS完成TCP、IP、MAC层封装，送到网卡处以比特流形式送送

（4）经过网络传输，比特流到达服务器端，被服务器接收。

（5）服务器OS逐一去掉 MAC、IP、TCP层封装，剥出应用层数据，也就是Request请求，并交给应用层的Web应用

（6）Web解析Request请求内容，并生成Respond响应，交给服务器OS

（7）Respond响应也是应用层数据，由服务器OS完成TCP、IP、MAC层封装，送到网卡处以比特流形式送送

（8）经过网络传输，比特流到达服务器端，被用户机器接收。

（9）用户机器OSS逐一去掉 MAC、IP、TCP层封装，剥出应用层数据，也就是Respond响应，并交给应用层的浏览器。

（10）浏览器根据Response响应内容，组织显示给用户看。


【域名解析过程】

当一个用户在地址栏输入www.taobao.com时，DNS解析【域名解析过程】有大致十个过程，基于UDP协议，如下：


1. 浏览器先检查自身缓存中有没有被解析过的这个域名对应的ip地址，如果有，解析结束。同时域名被缓存的时间也可通过TTL属性来设置。


2. 如果浏览器缓存中没有（专业点叫还没命中），浏览器会检查操作系统缓存中有没有对应的已解析过的结果。而操作系统也有一个域名解析的过程。在windows中可通过c盘里一个叫hosts的文件来设置，如果你在这里指定了一个域名对应的ip地址，那浏览器会首先使用这个ip地址。但是这种操作系统级别的域名解析规程也被很多黑客利用，通过修改你的hosts文件里的内容把特定的域名解析到他指定的ip地址上，造成所谓的域名劫持。所以在windows7中将hosts文件设置成了readonly，防止被恶意篡改。


3.  如果至此还没有命中域名，才会真正的请求本地域名服务器（LDNS）来解析这个域名，这台服务器一般在你的城市的某个角落，距离你不会很远，并且这台服务器的性能都很好，一般都会缓存域名解析结果，大约80%的域名解析到这里就完成了。


4. 如果LDNS仍然没有命中，就直接跳到Root Server 域名服务器请求解析


5. 根域名服务器返回给LDNS一个所查询域的主域名服务器（gTLD Server，国际顶尖域名服务器，如.com .cn .org等）地址


6. 此时LDNS再发送请求给上一步返回的gTLD


7. 接受请求的gTLD查找并返回这个域名对应的Name Server的地址，这个Name Server就是网站注册的域名服务器


8. Name Server根据映射关系表找到目标ip，返回给LDNS


9. LDNS缓存这个域名和对应的ip


10. LDNS把解析的结果返回给用户，用户根据TTL值缓存到本地系统缓存中，域名解析过程至此结束


ARP协议是用来将IP地址转换为MAC地址的对照表。

TCP/IP协议将应用层，表示层，会话层统称为应用层

应用层：传输单位为报文，各种APP，浏览为的协议，HTTP，FTP，


表示层：


会话层：


传输层：传输单位为报文段，进行数据分段。IP+PORT


网络层：传输单位为数据包，专门处理网络中数据包。


数据链路层：传输单位为数据帧。


物理层：传输单位为比特流[0,1指令]，例如电信号，光信号。




表示层（Presentation Layer）是OSI模型的第六层，它对来自应用层的命令和数据进行解释，对各种语法赋予相应的含义，并按照一定的格式传送给会话层。其主要功能是“处理用户信息的表示问题，如编码、数据格式转换和加密解密”等。
表示层的具体功能如下：
数据格式处理：协商和建立数据交换的格式，解决各应用程序之间在数据格式表示上的差异。
数据的编码：处理字符集和数字的转换。例如由于用户程序中的数据类型（整型或实型、有符号或无符号等）、用户标识等都可以有不同的表示方式，因此，在设备之间需要具有在不同字符集或格式之间转换的功能。
压缩和解压缩：为了减少数据的传输量，这一层还负责数据的压缩与恢复。
数据的加密和解密：可以提高网络的安全性。


会话层（Session Layer）是OSI模型的第5层，是用户应用程序和网络之间的接口，主要任务是：向两个实体的表示层提供建立和使用连接的方法。将不同实体之间的表示层的连接称为会话。因此会话层的任务就是组织和协调两个会话进程之间的通信，并对数据交换进行管理。
用户可以按照半双工、单工和全双工的方式建立会话。当建立会话时，用户必须提供他们想要连接的远程地址。而这些地址与MAC（介质访问控制子层）地址或网络层的逻辑地址不同，它们是为用户专门设计的，更便于用户记忆。域名（DN）就是一种网络上使用的远程地址例如：www.3721.com就是一个域名。
会话层的具体功能如下：
会话管理：允许用户在两个实体设备之间建立、维持和终止会话，并支持它们之间的数据交换。例如提供单方向会话或双向同时会话，并管理会话中的发送顺序，以及会话所占用时长。
会话流量控制：提供会话流量控制和交叉会话功能。
寻址：使用远程地址建立会话连接。
出错控制：从逻辑上讲会话层主要负责数据交换的建立、保持和终止，但实际的工作却是接收来自传输层的数据，并负责纠正错误。会话控制和远程过程调用均属于这一层的功能。但应注意，此层检查的错误不是通信介质的错误，而是磁盘空间、打印机缺纸等类型的高级错误。


OSI下3层的主要任务是数据通信，上3层的任务是数据处理。


传输层（Transport Layer）是OSI模型的第4层。因此该层是通信子网和资源子网的接口和桥梁，起到承上启下的作用。
该层的主要任务是：向用户提供可靠的端到端的差错和流量控制，保证报文的正确传输。传输层的作用是向高层屏蔽下层数据通信的细节，即向用户透明地传送报文。该层常见的协议：TCP/IP中的TCP协议、Novell网络中的SPX协议和微软的NetBIOS/NetBEUI协议。
传输层提供会话层和网络层之间的传输服务，这种服务从会话层获得数据，并在必要时，对数据进行分割。然后，传输层将数据传递到网络层，并确保数据能正确无误地传送到网络层。因此，传输层负责提供两节点之间数据的可靠传送，当两节点的联系确定之后，传输层则负责监督工作。综上，传输层的主要功能如下：
传输连接管理：提供建立、维护和拆除传输连接的功能。传输层在网络层的基础上为高层提供“面向连接”和“面向无接连”的两种服务。
处理传输差错：提供可靠的“面向连接”和不太可靠的“面向无连接”的数据传输服务、差错控制和流量控制。在提供“面向连接”服务时，通过这一层传输的数据将由目标设备确认，如果在指定的时间内未收到确认信息，数据将被重发。
监控服务质量:。


网络层（Network Layer）是OSI模型的第三层，它是OSI参考模型中最复杂的一层，也是通信子网的最高一层。它在下两层的基础上向资源子网提供服务。其主要任务是：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。该层控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。具体地说，数据链路层的数据在这一层被转换为数据帧，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。
一般地，数据链路层是解决同一网络内节点之间的通信，而网络层主要解决不同子网间的通信。例如在广域网之间通信时，必然会遇到路由（即两节点间可能有多条路径）选择问题。 

在实现网络层功能时，需要解决的主要问题如下：
 寻址：数据链路层中使用的物理地址（如MAC地址）仅解决网络内部的寻址问题。在不同子网之间通信时，为了识别和找到网络中的设备，每一子网中的设备都会被分配一个唯一的地址。由于各子网使用的物理技术可能不同，因此这个地址应当是逻辑地址（如IP地址）。
 交换：规定不同的信息交换方式。常见的交换技术有：线路交换技术和存储转发技术，后者又包括报文交换技术和分组交换技术。
 路由算法：当源节点和目的节点之间存在多条路径时，本层可以根据路由算法，通过网络为数据分组选择最佳路径，并将信息从最合适的路径由发送端传送到接收端。
 连接服务：与数据链路层流量控制不同的是，前者控制的是网络相邻节点间的流量，后者控制的是从源节点到目的节点间的流量。其目的在于防止阻塞，并进行差错检测。


数据链路层（Data Link Layer）是OSI模型的第二层，负责建立和管理节点间的链路。该层的主要功能是：通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。
在计算机网络中由于各种干扰的存在，物理链路是不可靠的。因此，这一层的主要功能是在物理层提供的比特流的基础上，通过差错控制、流量控制方法，使有差错的物理线路变为无差错的数据链路，即提供可靠的通过物理介质传输数据的方法。
该层通常又被分为介质访问控制（MAC）和逻辑链路控制（LLC）两个子层。

MAC子层的主要任务是解决共享型网络中多用户对信道竞争的问题，完成网络介质的访问控制；

LLC子层的主要任务是建立和维护网络连接，执行差错校验、流量控制和链路控制。
数据链路层的具体工作是接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层；并且，还负责处理接收端发回的确认帧的信息，以便提供可靠的数据传输。


在OSI参考模型中，物理层（Physical Layer）是参考模型的最低层，也是OSI模型的第一层。
物理层的主要功能是：利用传输介质为数据链路层提供物理连接，实现比特流的透明传输。
物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。


各个层数据过程：相当于每一层添加了自己这一层的首部之后，数据传给下一层。依次传到了物理层。到达目的地后，再一层一层解密获取数据。

首先应用层将数据报文按照协议封装格式压缩然后传递给传输层、

传输层通过协议将数据报封装为数据报段、然后传递给网络层，

网络层将数据报段封装为数据包，并传递给数据链路层，

数据链路层收到数据包，封装为数据帧，然后又将数据帧转比特流传递给物理层，

物理层将比特流通过光或电信号发送给目标。


Q：HTTPS理解，
   是以安全为目标的HTTP通道，简单讲是HTPP安全版，在HTTP加入SSL/TLS层【加密的详细内容就需要这一安全层】。
   主要作用：建立一个信息安全通道来保证数据传输的安全，确认网站的安全性。


Q：SSL/TLS的理解
   通俗讲，SSL[secure sockets layer]安全套接层，TLS是SSL的继任者，叫传输层安全[transport layer security]，就是在明文的上层[http都是明文传输]与TCP层之间加上一层加密，然后在这层之上承载HTTP，
   这样保证上层信息传输安全。


Q：一个扫描登录的业务流程。
   1，前端从服务器获取登录二维码,eg.点击电脑网页版登录微信，有一个二维码出现
   2，前端发请求给服务器校验用户是否登录[前端js定时发送登录校验请求]，这个二维码的作用
   3，已经登录的手机用户扫描二维码进行扫码登录，扫码操作就提取用户相关的信息然后发送请求传递给服务器，理解为手机端微信扫一扫二维码
   4，服务器收到手机扫描登录请求，提取用户信息，执行登录操作[generate cookie, session等信息]
   5，服务器收到浏览器端的登录校验请求，提取校验用户对象的cookie, session信息是否存在
   6，服务器返回用户登录成功的信息给浏览器 
   7，浏览器获取对应的登录信息写入本地 cookie，跳转页面完成登录。


Q：虚拟机与容器的区别
   容器是一个应用层抽象，用于将代码和依赖资源打包在一起。多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行。与虚拟机相比，容器占用的空间较少
  （容器镜像大小通常只有几十兆），瞬间就能完成启动。

   虚拟机(VM)是一个物理硬件层抽象，用于将一台服务器变成多台服务器。管理程序允许多个VM在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件以及库资源，因此占用大量空间。而且VM启动也十分缓慢。


Q：快速排序算法的时间复杂度分析
   快速排序基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行,以此达到整个数据变成有序序列。

   快速排序也是一种分治的递归算法。它的平均运行时间是O(NlogN)，最坏情形性能为O(N2)。
   为了分析快速排序的时间复杂度，请先看主定理: T [n] = aT[n/b] + f (n)
  

Q：数组以及链表的区别/时间复杂度
   数组：元素在内存中连续存放，理解为一块连续的区域。由于每个元素占用内存相同，内存是连续的，可以通过下标迅速访问数组中任何元素[从数组的首地址向后偏移]。

   如果要在数组中增加一个元素，待插入位置的的元素和它后面的所有元素都需要向后搬移，在内存中空出一个元素的空间，然后将要增加的元素放在其中。同样的道理，
   如果想删除一个元素，待删除位置后面的所有元素都需要向前搬移。如果应用需要快速访问数据，很少插入和删除元素，就应该用数组，随机访问效率很高，时间复杂度可以达到O(1)。

   在使用前需要提前从栈分配所占内存的大小，这样不知道需要多大的空间就预先申请可能会浪费内存空间，即数组空间利用率低。数组的空间在编译阶段就需要进行确定，所以需要
   提前给出数组空间的大小（在运行阶段是不允许改变的）。空间在不够使用的时候需要扩容，扩容的话，就会涉及到需要把旧数组中的所有元素向新数组中搬移。


   链表：元素在内存中不是顺序存储的，分散的不需要连续，而是通过存在元素中的指针联系到一起，通过地址找到下一个数据。每个元素包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个元素地址的指针域。

   因为链表的空间是分散的，所以不具有随机访问性。如果要访问链表中一个元素，需要从第一个元素开始找起，依次往后遍历一直找到需要的元素位置。在查找某个元素时，
   时间复杂度为O(N)。
   如果增加和删除一个元素，对于链表数据结构就非常简单了[时间复杂度为O(1)]，只要修改元素中的指针就可以了。
   如果应用需要经常插入和删除元素，而对访问元素时的效率没有很高要求的话就需要用链表。

   使用前空间不需要提前指定大小，是动态申请的，根据需求动态的申请和删除内存空间，扩展方便，故空间的利用率较高。链表的空间是从堆中分配的。


Q：如何判断一个链表是否有环，如果有怎么确定链表的头


Q：栈与堆的区别[两者都是数据项按序排列的数据结构]
   栈，是一个内存数组，是一个LIFO（last in first out 后进先出）的数据结构。
   堆，是一块内存区域，用于储存类型的数据对象。常用来实现优先队列，堆的存取是随意的。


   我们现在经常用的并不是数据结构中的堆和栈，之所以说了数据结构中的堆和栈是为了与堆区和栈区区别开来，请大家一定要注意。
1、栈区（stack）— 由编译器自动分配释放[不指定数据的大小]，存放函数的参数值，局部变量的值等，其操作方式类似于数据结构中的栈。
2、堆区（heap） — 一般由程序员分配释放[指定数据的大小]， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。
3、全局区（静态区）（static）—，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块
   区域。 - 程序结束后由系统释放 
4、文字常量区—常量字符串就是放在这里的。 程序结束后由系统释放
5、程序代码区—存放函数体的二进制代码。

请参考如下的定义：
//main.cpp 
int a = 0; 全局初始化区 
char *p1; 全局未初始化区
 
main() 
{ 
int b; 栈 
char s[] = "abc"; 栈 
char *p2; 栈 
char *p3 = "123456"; 123456\0在常量区，p3在栈上。 
static int c =0； 全局（静态）初始化区 
p1 = (char *)malloc(10); 
p2 = (char *)malloc(20); 
分配得来得10和20字节的区域就在堆区。 
strcpy(p1, "123456"); 123456\0放在常量区，编译器可能会将它与p3所指向的"123456"优化成一个地方。 
}  

stack: 由系统自动分配。 例如，声明在函数中一个局部变量 int b; 系统自动在栈中为b开辟空间。不需要指定大小，由系统自动分配。 
heap: 需要程序员自己申请，并指明大小，在c中malloc函数 
如p1 = (char *)malloc(10); 
在C++中用new运算符 
如p2 = (char *)malloc(10); 
但是注意p1、p2本身是在栈中的。 


只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。 
栈内存:存储的都是局部变量。先加载函数才能进行局部变量的定义，所以方法先进栈，然后再定义变量，变量有自己的作用域，一旦离开作用域，变量就会被释放。栈内存的更新速度很快，因为局部变量的生命周期都很短。


堆内存:存储的是数组和对象（其实数组就是对象），凡是new建立的都是在堆中，堆中存放的都是实体（对象），实体用于封装数据，而且是封装多个（实体的多个属性），
      如果一个数据消失，这个实体也没有消失，还可以用，所以堆是不会随时释放的，但是栈不一样，栈里存放的都是单个变量，变量被释放了，那就没有了。堆里的实体虽然不会被释放，但是会被当成垃圾，垃圾回收机制不定时的收取。


一般一个函数运行的过程如下，先执行主函数里第一个命令语句，主函数先进栈，在栈中定义一个变量arr,接下来为arr赋值，但是右边不是一个具体值，是一个实体。实体创建在堆里，在堆里首先通过new关键字开辟一个空间，内存在存储数据的时候都是通过地址来体现的，地址是一块连续的二进制，然后给这个实体分配一个内存地址。数组都是有一个索引，数组这个实体在堆内存中产生之后每一个空间都会进行默认的初始化（这是堆内存的特点，未初始化的数据是不能用的，但在堆里是可以用的，因为初始化过了，但是在栈里没有），不同的类型初始化的值不一样。所以堆和栈里就创建了变量和实体：


那么堆和栈是怎么联系起来的呢?

     我们刚刚说过给堆分配了一个地址，把堆的地址赋给arr，arr就通过地址指向了数组。所以arr想操纵数组时，就通过地址，而不是直接把实体都赋给它。这种我们不再叫他基本数据类型，而叫引用数据类型。称为arr引用了堆内存当中的实体。基本类型的变量在栈的局部变量表中直接存储的具体数值，而对象类型的变量则存储的堆中引用。显然，相对于基本类型的变量来说，对象类型的变量需要占用更多的内存空间。八种基本数据类型，包括：byte，int，long，double，float，boolean，char，short。基本类型与对象类型最大的不同点在于，基本类型基于数值，对象类型基于引用。

如果当int [] arr=null;

              arr不做任何指向，null的作用就是取消引用数据类型的指向。

              当一个实体，没有引用数据类型指向的时候，它在堆内存中不会被释放，而被当做一个垃圾，在不定时的时间内自动回收，因为Java有一个自动回收机制，（而c++没有，需要程序员手动回收，如果不回收就越堆越多，直到撑满内存溢出，所以Java在内存管理上优于c++）。自动回收机制（程序）自动监测堆里是否有垃圾，如果有，就会自动的做垃圾回收的动作，但是什么时候收不一定。

             所以堆与栈的区别很明显：

            1.栈内存存储的是局部变量而堆内存存储的是实体；

            2.栈内存的更新速度要快于堆内存，因为局部变量的生命周期很短；

            3.栈内存存放的变量生命周期一旦结束就会被释放，而堆内存存放的实体会被垃圾回收机制不定时的回收。


go语言编译器自动将一个变量放在栈or堆，编译器做逃逸分析，当发现变量的作用域没有跑出函数范围，就可以分配在栈上，反之必须分配在堆上，所以不用担心编译时内存泄漏。

go程在运行有可能有因为没被回收导致内存泄露。


函数返回局部变量的地址，局部变量内存分配在栈空间，因为函数返回后，系统自动回收函数里定义的局部变量，所以运行时访问一个被系统回收后的地址空间，就一定会发生错误，内存分配在堆中即可。




Q：哈希表..原理

哈希表，hastable，也可以叫做散列表，名字不重要，只要明白这两者均代表同一样就对了。它是一种可以通过关键码值而可以直接快速访问的数据结构。

什么是哈希表（散列表）？

哈希表（散列表）是一系列关键码值通过哈希函数（散列函数）计算后映射在一个连续有限的区间上的集合，这个集合成为哈希表（散列表），这个集合保存了这一系列关键码值的散列地址。给定这一系列关键码值中的任意一个关键码值可以快速定位到对应的散列地址。

什么是关键码值（关键字）？

关键码值（关键字）对于哈希函数（散列函数）来说就是一个可变因子，相当于我们常说的变量，例如f(x)=ax+b，通过将x带入函数式中产生后的结果“象”映射在哈希表中，这个象也称为改关键码值的散列地址。

什么是哈希函数（散列函数）？

哈希函数（散列函数）作为关键码值与哈希表的中间介质，关系着关键码值产生的哈希地址，选取合适的哈希函数有着重要意义，良好恰当的哈希函数能够提高哈希表的访问性能，减少哈希冲突，增加数据的访问速度。

什么是哈希冲突（散列冲突）？

哈希冲突（散列冲突）顾名思义就是在构造哈希表时产生的冲突，假如在没有设计良好的哈希函数或没有选取恰当的解决冲突的方法的情况下，不同的关键码值可能产生相同的结果，即映射到相同的散列地址，这就是哈希冲突。(x1不等于x2，但f(x1) = f(x2))。

设计良好哈希函数应该考虑哪些因素？

    哈希函数计算所耗时间
    关键码值的长度
    关键码值的分布情况
    哈希表的大小
    记录的访问频率
哈希表解决冲突的办法主要有两种：

开放寻址法：（open addressing）
下面介绍其中一种，叫线性探测（linear probing）（ThreadLocalMap)
插入：
当插入遇到冲突的时候，这种方法会一直往后面找，知道遇到空闲的位置才把值插入到相应位置。
查找：
查找的时候会先哈希，得到哈希值以后就去相应的下标找数据，如果下标的为位置和你输入的key是一致的，则找到了，如果不一致则继续往下找，知道找到为止。
删除：
当删除的时候，首先要找到相应的值，然后把那个位置标记成deleted。如果不标记成deleted而是把那个位置标记空的话，那有可能下次查找的时候会出现错误，本来有的会找不到。

链表法：（chaining）(LinkedHashMap)
因为每个槽中存储的都是对应数据的链表，所以当出现冲突的时候，我们只需要在当前位置的链表后面增加一个元素即可。

装载因子和动态扩容：
装载因子的大小为 factor = n / m，其中n为当前哈希表有多少个元素，m为哈希表的总的容量。
当装载因子超过事先预设好的阈值后，哈希表就会启动动态扩容。动态扩容有两种方式，一种是一次性扩容，还有一种是分散性扩容。
一次性扩容：
当插入一个新的数据的时候，如果装载因子超过了设定的阈值，则启动动态扩容，这个时候会新申请一个原来容量2倍的哈希表，然后把原来哈希表里面的数据全部重新哈希放到新的哈希表里面去，这样的时间复杂度是O(N)。如果对于性能要求很高的场景，显然这种方式是不合适的。
分散性扩容：
申请一个原来2倍大小的哈希表，把新的数据插入到新的哈希表中，再从老的哈希表里选一个出来插入新的哈希表，直到所有老的哈希表里的数据全部搬到新的哈希表里。对于查找操作来说，先查找新的哈希表再找老的哈希表。这样把动态扩容的时间分摊到每一次的插入操作里，就不会出现时间复杂度为O(N)的情况。可以满足性能要求高的场景。

开放寻址法和链表法优缺点对比：
开放寻址法的缺点：
开放寻址法适合装载因子小，且数据量比较小的场景，因为当装载因子过大的话哈希表的冲突的概略会很大，所以每次查找时间复杂度在极端情况下会退化到O(N)。开放寻址发是开辟一块连续的内存空间，如果数据量很大的话，对内存的利用率不高。
开放寻址发优点：
因为它是连续的一块内存，所以对cpu缓存非常友好，而且这要会对数据结构的序列化有很大的好处。

链表法缺点：
因为里面存储着指针，而且是用链表存储的，所以它不是连续的一块内存，所以会对序列化不利，而且对cpu缓存不友好。
链表法优点：
当出现冲突的时候，只是在链表后面添加或查找元素，在极端的情况下，不用搜索整个哈希表来查找元素，所以适合用来存储大量数据。而且利用链表来存储数据对内存的利用率很高。在某些情况下链表可以该用成红黑树，跳表的数据结构，更进一步加快了插入查找的效率。

如何设计一个好的哈希表：
对于设计一个好的哈希表要考虑到3点要素，哈希函数，冲突解决，装载因子。
哈希函数：
哈希函数要尽量保证分布均匀，不要太耗时，否则对于频繁访问哈希表的场景来说会非常耗时。
冲突解决：
冲突解决上面已经提到过，根据不同的场景使用不同的解决方案。对于链表法的哈希表，我们可以把链表改成红黑树，这样当一个链表很长的情况下也保证了效率。
装载因子：
对于内存很吃紧的情况，我们可以把装载因子设置成大一点，但是在利用开放寻址法的哈希表中，装载因子不能设置成很大，否则发生的冲突概率会很大，效率会极速下降。对于内存消耗不是很敏感的场景，可以设置成小一点，这样哈希表的性能会得到保证。


Q：hashMap的实现原理
   数组按照下标查找元素的时间复杂度是O(1)，map是根据这个来实现的，是数组加上链表的组合实现的，具体baidu


Q：钓鱼网站，socket处理time.wait？
   
https://blog.csdn.net/yangwohenmai1/article/details/92620987
https://blog.csdn.net/qingzhuyuxian/article/details/83242620


Q：怎么保证小数点后16位精度
   这个暂时没找到，不过找到个float64精度丢失的情况，参考一下流程处理，这些方法应该都是根据小数点后面一位来四舍五入，实际案例应该有比较多的情况：
   //方法1： 
   tmpStr1 := fmt.Sprintf("%.2f", a) 
   tmpStr2 := fmt.Sprintf("%.2f", b) 
   tmpnum1, _ := strconv.ParseInt(strings.Replace(tmpStr1, ".", "", 1), 10, 64) 
   tmpnum2, _ := strconv.ParseInt(strings.Replace(tmpStr2, ".", "", 1), 10, 64) 
   c = tmpnum1 + tmpnum2 fmt.Printf("第3次 c=%d\n", c)

   //方法2：
   strFloat := strconv.FormatFloat(a*100, ‘f’, 0, 64)
    nInt64, err := strconv.ParseInt(strFloat, 10, 64)

   //方法3：
    num3 := math.Ceil(a * 100.0)
   fmt.Printf(“num3=%d\n”, int64(num3)) //num3=255


Q：递归，循环之间关系？
  递归应该都可以写成循环，循环不一定可以写成递归。


Q：内存泄漏？
  https://blog.csdn.net/hizhangyuping/article/details/80624506


Q：设计模式有哪些？go语言怎么写一个单例模式
   https://blog.csdn.net/qq_33706840/article/details/81631762


Q：如何维护一个长连接，怎么写一个tcp连接的服务器？怎么设计一个高并发的大型服务器？
   https://my.oschina.net/woyaoxue/blog/1603016
   https://blog.csdn.net/weixin_34203832/article/details/92030906


Q：项目中最困难的地方，or项目里自己做的最自豪的事情？


Q：MQ队列的一些面试总结。与此相关联的问题有：熟悉常用开源中间件服务？
   sender -->  Mwssage Queue --> receiver

消息队列的出现用于完成系统间的消息通信，协调系统调用间调用。这跟SOA类似。但不同于SOA面向服务的直接调用，消息队列的通信不是直接调用关系，系统间通信主要通过消息发送，接收方接收消息，进行处理，完成与发送方的调用处理。

   MQ，消息队列（例如kafka）是跨主机，跨进程的通讯机制【例如：cobol与WTS的queue】，chan是单进程里or线程间的通讯消息队列。消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯;比如实现点对点消息队列，或者聊天室等。点对点通讯

   为什么引入MQ？--》系统解耦，异步调用，流量削峰。

   保证中间键崩溃了数据不丢失？--》把消息持久化写入到磁盘。

   如何保证消息不被重复消费？--》根据主键先查询一下，如果数据有了就别插入了，update。用set cache命令写redis，用全局唯一的id，数据库唯一键；

   kafak如果丢了数据怎么办？--》要提前设置四个参数：
   给 topic 设置 replication.factor 参数：这必须大于 1，要求每个 partition 必须有至少 2 个副本。
   在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
   在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。
   在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。

   保证消息顺序性？--》实际上rabbitMQ, kafka，差不多都用多个内存queue来保证顺序性。每个事物或者线程用自己的queue


1.如果你的数据库没有读写分离，缓存的是单条数据的缓存，例如用户的信息，订单信息等。选择先删除缓存，再更新数据库。

2.读写分离，分表分库的情况下，推荐使用先更新数据库，再删缓存

无论是先更新数据库，再删缓存还是先删除缓存，再更新数据库，都涉及二次缓存删除可靠性的问题，先更新数据库，再删缓存最终缓存得到的脏数据概率更低

3.缓存信息是经过大量计算出来的，例如公司运营数据，报表数据。

这种情况要考虑你的业务的正确性，如果经过大量计算出来缓存到内存的内容经常需要变更，那为什么需要缓存呢？

最后再切记：一开始考虑去缓存经常被更新的数据，从业务上就是一个错误


Q：redis怎么扩容
   redis集群或者加缓冲池，护着直接说运维做的


Q：context上下文包的作用？
   Go 中的 context 包在与 API 和慢处理交互时可以派上用场，特别是在生产级的 Web 服务中。在这些场景中，您可能想要通知所有的 goroutine 停止运行并返回。使用 context 可以
   置截止日期，超时或调用取消函数来通知所有使用任何派生 context 的函数来停止运行并返回。

    context.Background 只应用在最高等级，作为所有派生 context 的根。
    context.TODO 应用在不确定要使用什么的地方，或者当前函数以后会更新以便使用 context。
    context 取消是建议性的，这些函数可能需要一些时间来清理和退出。
    context.Value 应该很少使用，它不应该被用来传递可选参数。这使得 API 隐式的并且可以引起错误。取而代之的是，这些值应该作为参数传递。
    不要将 context 存储在结构中，在函数中显式传递它们，最好是作为第一个参数。
    永远不要传递不存在的 context 。相反，如果您不确定使用什么，使用一个 ToDo context。
    Context 结构没有取消方法，因为只有派生 context 的函数才应该取消 context。


main 函数

    用 cancel 创建一个 context
    随机超时后调用取消函数

doWorkContext 函数

    派生一个超时 context
    这个 context 将被取消当
        main 调用取消函数或
        超时到或
        doWorkContext 调用它的取消函数
    启动 goroutine 传入派生上下文执行一些慢处理
    等待 goroutine 完成或上下文被 main goroutine 取消，以优先发生者为准

sleepRandomContext 函数

    开启一个 goroutine 去做些缓慢的处理
    等待该 goroutine 完成或，
    等待 context 被 main goroutine 取消，操时或它自己的取消函数被调用

sleepRandom 函数

    随机时间休眠
    此示例使用休眠来模拟随机处理时间，在实际示例中，您可以使用通道来通知此函数，以开始清理并在通道上等待它，以确认清理已完成。




Q：Webservice以及RMQ？


Q：连续上传10张图片，限制50M以内，怎么统计？
   http请求头里有个统计数据流的方法

   上传图片判断一下后缀以及大小就可以了么，怎么避免上传不是图片后缀为.jpg的文件
   可以把存储照片的文件夹设置为不可执行就可以避免被攻击。


------------------------------------------------------算法这些的问题：


Q：有一个有序数组，被循环右移了若干步，求数组最小值。
	

数组的内存是连续的，循环右移：一个数组A中存有I（I>0）个整数，在不允许使用另外数组的前提下，将每个整数循环向右移M（M>=0）个位置，即将A中的数据由（A0 A1……A[I-1]）变换为（A[I-M] …… A[I-1] A0 A1……AN-M-1）（最后M个数循环移至最前面的M个位置）。eg. 0,1,2,3,4,5,6,7,8,9 向右循环移动2位变成：8,9,1,2,3,4,5,6,7.这个时候直接遍历数组即可，时间复杂度为O(n)，遍历到当前元素比上一个小，就是最小值了。
如果是有序数组，可以采用二分查找方法。




Q：通用的冒泡排序，如果是结构体排序了？


Q：二叉树，左小右大，写一个insert方法，find方法？如果用go怎么将insert,find做并发操作？


Q：单项链表倒置，revert？



------------------------------------------------------比特币问题：

Q：比特币的难度值怎么动态产生？


Q：区块链怎么实现分布式存储以及去中心化问题


Q：Utxo是什么？比特币的存储问题？


Q：用户名与密码能否实现DApp的账户功能？为什么？有哪些方法可以实现DApp的用户账户？


Q：DApp如果需要从中心化应用中获取数据该怎么做？


Q：公钥，私钥加密机制？
   例如 ：RSA加密
生成私钥操作流程
	1. 使用rsa中的GenerateKey方法生成私钥
	2. 通过x509标准将得到的ras私钥序列化为ASN.1 的 DER编码字符串
	3. 将私钥字符串设置到pem格式块中
	4. 通过pem将设置好的数据进行编码, 并写入磁盘文件中


生成公钥操作流程
	1. 从得到的私钥对象中将公钥信息取出
	2. 通过x509标准将得到 的rsa公钥序列化为字符串
	3. 将公钥字符串设置到pem格式块中
	4. 通过pem将设置好的数据进行编码, 并写入磁盘文件


公钥加密
	1. 将公钥文件中的公钥读出, 得到使用pem编码的字符串
	2. 将得到的字符串解码
	3. 使用x509将编码之后的公钥解析出来
	4. 使用得到的公钥通过rsa进行数据加密


私钥解密
	1. 将私钥文件中的私钥读出, 得到使用pem编码的字符串
	2. 将得到的字符串解码
	3. 使用x509将编码之后的私钥解析出来
	4. 使用得到的私钥通过rsa进行数据解密


Q：比特币才用椭圆曲线加密算法来产生公钥，私钥，钱包地址即公钥，私钥由用户保存
   

Q：非对称加密中使用的主要算法有：RSA，Elgamal，背包算法，Rabin，D-H，ECC(椭圆曲线加密算法)，非对称加密的缺点为加密和解密花费时间长，速度慢，只适合对少量数据进行加密


Q：常用的hash算法为：MD5，SHA1，SHA256，SHA512，HMAC等，相比非对称加密，hash算法一般快几个数量级以上。


Q：点对点通讯技术，纯P2P网络模型，分层式P2P模型？


Q：分布式共识技术，FLP不可能性定理，CAP理论，Paxos分布式共识算法，Raft算法，PBFT拜占庭攻击容错算法，POW，POS，DPOS？


Q：区块链分为私有区块链，公共区块链，联盟区块链，完全私有区块链？主链与测链的特性[这个应该是分叉问题吧]？


Q：图灵完备的编程语言，内置了编程语言的区块链协议。一个可以计算所有图灵机可计算函数的计算系统成为图灵完备。图灵不完备更安全，图灵完备更智能。


Q：以太坊上每一笔交易都被收取一定量的Gas(油费)，目的是限制执行交易所需的工作量，同时为执行支付费用。


Q：自己银行系统转账当然快了，跨行应该是涉及到中国人名银行这种中间机构，SWIFT应该是跨区域，跨国转账吧


Q：Riper协议及Riper币？


Q：比特币的工作量证明机制让账本需要六次确认


Q：Hyperledger是采用了类似Ripple的共识机制，交易确认过程可在几秒钟之内完成，达成共识则是通过拜占庭容错算法机制。


Q：geth是以太坊的go语言客户端，安装了geth客户端，创建账号就是在终端执行geth account new命令


Q：如何创建fabric网络


Q：如何创建私有链【或者创建代币】


Q：如何创建私有测试网


Q：以太坊创建账号额过程是怎样？


Q：


Q：


Q：


Q：




项目业务问题：

Q：下单业务，更新user表，book表，考虑事务，并发安全

beego里面前台怎么传数据给后台？

*  在html页面代码里，form标签里有个method属性为POST并且确定对应的栏位名称，在beego里面用GetString这些方法进行接收，可以获取页面栏位里输入的值。

*  将表单里面的内容直接解析到一个struct中，完成数据的POST。后台需要新建一个struct去接收前端的数据。这里用到了structTag,定义一个结构体，属性后面加form标签，用ParseForm(&u)直接获取前台页面值
   定义 struct 时，字段名后如果有 form 这个 tag，则会以把 form 表单里的 name 和 tag 的名称一样的字段赋值给这个字段，否则就会把 form 表单里与字段名一样的表单内容赋值给这个字段。
   如上面例子中，会把表单中的 username 和 age 分别赋值给 user 里的 Name 和 Age 字段。调用 Controller ParseForm 这个方法的时候，传入的参数必须为一个 struct 的指针，
   否则对 struct 的赋值不会成功并返回 xx must be a struct pointer 的错误。如果要忽略一个字段，有两种办法，一是：字段名小写开头，二是：form 标签的值设置为 -

*  获取JSON / Request Body 里的内容。在企业中我们用的更多的是API开发，而API中的数据经常会用到 JSON 或 XML 来作为数据交互的格式。这是如何实现的交互，我们来感受一下。

   在配置文件里设置 copyrequestbody = true
   在 Controller 中
   json 用来解析JSON的包
   ob定义的struct

   func (this *ObjectController) Post() {
        var ob models.Object
        json.Unmarshal(this.Ctx.Input.RequestBody, &ob)
        objectid := models.AddOne(ob)
        this.Data["json"] = "{\"ObjectId\":\"" + objectid + "\"}"
        this.ServeJSON()
   }


订单业务：
[有一种方案：用户并发请求到达服务器时，创建订单先，然后扣除库存，等待用户支付]

只要创建订单就要频繁操作数据库IO，有个不直接操作数据库IO的方案是预扣库存：先扣除库存，保证不超卖，然后异步产生用户订单，这样相应给用户的速度就会快很多，订单有有效期[五分钟不支付订单失效，一旦失效就加入新的库存]。订单是异步产生的额，一般会放到MQ，kafka这样的即时消费队列中处理，订单比较少的情况下，生成订单佛非常快，用户几乎不用排队


Web相关技术：
1.前后端分离(restful)，前端页面有前端服务器提供，数据通过ajax向后端服务请求
2.模板技术，beego中的模板
3.缓存技术(redis, memcache)
4.异步技术，web中的异步任务怎么处理，发送邮件短信等(celery)
5.图像技术，图片处理相关的
6.消息队列rabbitMQ，RocketMQ保证任务消息传递
7.认证技术，OAuth jwt保证安全问题
8.搜索技术，elesearch，golang版本
9.虚拟化，Docker主要用于项目打包部署
10.定时任务，go语言时间轮
11.消息推送，wensocket第三方平台
12.SEO优化技术，这个主要是增加搜索排名，可以找第三方做
13.日志处理，
14.文件存储，分布式文件存储或者对象存储
15.项目部署，


利用JSON字符串从页面相关栏位获取订单信息，购买数量，单价这些。然后开启事务，读取用户表看这个用户是否存在，查询用户信息是否存在，赋值给订单结构体，进行插入订单表操作，先看表里库存是否足够，不足直接按照实际库存返回，如果款村已经为0则回滚，并且同时同步缓存里面库存数量[因为前段页面有设置，查询得到库存为0则无法购买]，有库存则用o.QueryTable("GoodsSKU").Filter("Id",goods.Id).Filter("Stock",preCount).Update(orm.Params{"Stock":goods.Stock,"Sales":goods.Sales})方法自动join商品库存表来更新库存量，完成后就提交事务。


数据库的读写分离，主从复制看这个 https://www.cnblogs.com/codehome/p/9356496.html。应该是数据库层面设计的问题，我们开发实际上很多没有涉及？



用户请求----------》查询缓存是否有结果----------》第一次无缓存----------》查询数据库----------》返回写入缓存----------》然后返回用户
同步表与缓存：用户请求的时候更新，定时更新缓存，数据增加的时候更新缓存



购物车业务：
登陆了购物车信息在cookie，登陆了存储在redis

订单模块：

	

订单管理
[用户并发请求到达服务器时，先扣除库存，保证不超卖，然后异步产生用户订单，等待用户支付]


首先，app通过添加商品到购物车才可以下订单，在完成提交订单操作马上返回一个下单成功给用户，然后利用队列存储订单请求异步处理，后台从队列里根据提取到传递过来的订单数据，连接redis数据库根据商品id判断购物车里面是否有商品[=获取商品id的skuid，买几件]，防止出现能够直接提交订单的情况，如果返回结果为空则不能下订单，接着判断用户是否登录，如果没有登录把当前访问的地址存储到 session里，跳转登录页面，完成登录后再跳转回来，接着进一步根据商品id在商品表查询库存量，这里开启事务保证完整性，等于加了行锁直到购买完成，通过ORM映射关系查询商品表[可以直接用ORM方法或者ORM.EXEC(原生sql语句)]，查询库存量是否满足要求，不满足则回滚并且返回下单失败错误信息，满足则计算订单总额，并且生产唯一订单号，更新表里的库存量。然后返回数据，提交事务。接着跳到展示订单页，判断收货人信息是否填写，没有则提示填写。点击结算后调用支付接口完成支付，随后将订单信息，订单商品信息入库结算完成后，清空购物车，订单完结。






商品模块：
首页信息显示、商品详情、商品列表及分页、商品分类、商品搜索等；

1.首页页面由于数据变化不频繁，使用页面静态话技术生成静态模板，用户访问时路由直接定位到静态页面，缩减了响应时间，提高了用户体验。

2.商品列表的的信息显示，利用FastDFS搭建网站图片存储服务器，实现类似云空间的功能，根据搜索关键字快速返回网页结果，同时调用图片服务而不是等待一起返回结果，提升用户体验。


商品分页功能，根据搜索结果分页显示出全部结果，减少数据量过多的压力。




购物车模块：
购物车信息、购物信息编辑等；
未登录的购物车用Cookie存储，已经登录的购物车用Redis存储，从而实现用户未登录状态下以及登录状态下购物车合并，提高用户体验。
从页面获取商品的id,skuid，通过路由过滤器判断用户是否登录，未登录写入cookie，登录了则直接连接redis，获取原来的数量，然后将总数累加。购物车数据在redis，
用hash来存储购物车信息，常用的就是hset hget hdel key value方法。用户登录后，要判断 cookie 里面是否有购物车数据，如果有，则就移动到数据库，


cookie存储格式大概如下：

cookie:name=value;

例如下面：cookie的name就是_gads和_utma;value就是ID=b67xxxx015和22xxxx709.5。总数为两个,中间以封号分隔。

document.cookie;
"_gads=ID=b67e99f3b63cd9db:T=1438053105; _utma=226521935.1840446057.1438107104.1440702591.1446462709.

5;


秒杀模块: 
1，如果目标库存量过大，进行前后端分离，将秒杀页面静态化，通过CDN【内容分发网络】缓存以及缓存到用户浏览器，在开始前将秒杀按钮置灰色杜绝请求，设置按钮为一分钟点击五次，减少用户频繁发送请求的概率。

2，根据压力测试得到的Mysql最大连接数，利用队列在到达数据库之前过滤掉大量请求。只允许最大连接数范围内请求进入队列，多余请求则通过rabbitMQ call nack函数直接返回秒杀结束，请刷新或者稍后重试。这时候可以选择多加几台机器，不同类型的商品表放在不同机器上，设置有多个队列，每个负载均衡根据商品类型发送到不同机器，然后每个服务器的数据库使用连接池，超过的请求就等待。同事嘟列还可以接收其他请求。

3，通过的请求会超过库存量导致超卖，利用Mysql互斥锁依次更新库存量，当库存量为零则返回秒杀失败给用户。秒杀，成功的用户则返回成功页面，并且页面可以点击到订单页面，带有支付链接而不是直接跳至支付页面进一步减轻服务器压力。

























Docker部署问题：

请求经过负载nginx负载均衡----------》到达应用服务器----------》应用服务器缓存[集群]----------》数据库[集群]


如果用户量不是很大，一般集群就配置个主从即可用户备份数据


调试bug流程：遇到bug检查日志信息，根据日志的反馈信息排查问题，跟docker不是docker没关系，因为docker可以看做是一个小的服务，如果是，排查问题费事一些，一定要理清业务关系，根据业务线流程来排查


网站的的用户访问量，通过中间键来统计用户量，or 解析web日志统计流量


https://blog.csdn.net/yang731227?t=1

https://blog.csdn.net/qq_35190492/article/details/103105780

https://blog.csdn.net/qq_35190492/article/details/103153444?ops_request_misc=%7B%22request_id%22%3A%22158270244919725222463970%22%2C%22scm%22%3A%2220140713.130056874..%22%7D&request_id=158270244919725222463970&biz_id=0&utm_source=distribute.pc_search_result.none-task

https://blog.csdn.net/qq_35190492/article/list/2

https://blog.csdn.net/LookAtTheStars/article/details/51461491?ops_request_misc=%7B%22request_id%22%3A%22158271248119725256755333%22%2C%22scm%22%3A%2220140713.130056874..%22%7D&request_id=158271248119725256755333&biz_id=0&utm_source=distribute.pc_search_result.none-task





事务同步，事务隔离级别，UML，微服务【组件，控件】，扣款成功了，积分失败了怎么做？


















1m


channel



